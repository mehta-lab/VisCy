{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Image Translation\n",
    "***\n",
    "## Table of contents: <a class=\"anchor\" id=\"toc\"></a>\n",
    "\n",
    "1. [Overview](#overview)   \n",
    "2. [Setup](#setup)\n",
    "3. [Explore input and target images](#explore)\n",
    "4. [Metadata](#metadata)\n",
    "5. [Pilot: 2D image translation on a small dataset](#pilot)\n",
    "    * [Preprocessing](#preproc)\n",
    "    * [Training](#train)\n",
    "    * [Prediction](#predict)\n",
    "    * [Model tuning](#tuning)\n",
    "6. [Bonus: 2D image translation on a large dataset](#bonus)\n",
    "***\n",
    "This notebook illustrates image translation with deep convolutional neural networks (CNN). We frame the image translation as a regression problem solved using a residual U-Net model. The notebook demonstrates how to translate quantitative phase images of mouse kidney tissue to the fluorescence images of F-actin using data and model reported in our paper(https://doi.org/10.7554/eLife.55502)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Overview <a class=\"anchor\" id=\"overview\"></a>\n",
    "\n",
    "U-Net model consists of an encoder (downsampling) part and a decoder (upsampling) part. The U-Net model is immensely popular for many image analysis tasks. One of the key design feature of the U-Net architecture is the skip connections between the encoder and decoder layers, which allows the model to learn patterns at multiple spatial resolutions. U-Nets were orginally designed for image segmentation (https://link.springer.com/chapter/10.1007/978-3-319-24574-4_28). Using U-Nets for image translation (a regression problem) needs a few tweaks, such as: \n",
    "* changing the loss function, \n",
    "* changing the final activation layer, and \n",
    "* data normalization (more on that later).\n",
    "\n",
    "Below is an overview of the 2D and 2.5D residual U-Net model architecture. The main differences from the original U-Net architecture are: \n",
    "1. we add short skip connection within each block (residual block) in addtion to the long skip connections in the original U-Net. \n",
    "2. The 2.5D model provides better translation accuracy than 2D model - it uses 3D stacks as input and predicts 2D output. \n",
    "\n",
    "![U-Net architecture](supp_modelarch_RGB.png)\n",
    "\n",
    "The 2D model translates slice$\\rightarrow$slice, whereas 2.5D model translates stack$\\rightarrow$slice. \n",
    "\n",
    "We'll be using the architecture similar to above, but with same convolution instead of valid convolution.  Same convolution operation pads the input image so that the output image has the same size as the input image after convolution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BACK TO [TABLE OF CONTENTS](#toc)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup: libraries and paths <a class=\"anchor\" id=\"setup\"></a>\n",
    "\n",
    "Let's start with some imports. We'll be using the open source U-Net repository [microDL](https://github.com/czbiohub/microDL) , so we'll import that along with some plotting and other utility packages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System path: /home/christian.foley/virtual_staining/microDL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# autoreload modules before executing\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import glob\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import importlib\n",
    "import os\n",
    "import pandas as pd\n",
    "import pprint\n",
    "import sys\n",
    "\n",
    "# Add module path to sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "print(\"System path: \"+module_path)\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "import micro_dl.utils.meta_utils as meta_utils\n",
    "import micro_dl.cli.train_script as train\n",
    "import micro_dl.cli.preprocess_script as preprocess\n",
    "import micro_dl.utils.aux_utils as aux_utils\n",
    "import micro_dl.utils.image_utils as im_utils\n",
    "import micro_dl.utils.masks as mask_utils\n",
    "import micro_dl.utils.normalize as norm_utils\n",
    "import micro_dl.utils.train_utils as train_utils\n",
    "import micro_dl.inference.image_inference as image_inf\n",
    "import micro_dl.cli.metrics_script as metrics\n",
    "    \n",
    "# Setup pretty printing\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "\n",
    "font = {'family' : 'DejaVu Sans',\n",
    "        'size'   : 20}\n",
    "\n",
    "matplotlib.rc('font', **font)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define paths where input images are stored and where processed data and model will be saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This is the directory where our imaging data is downloaded\n",
    "INPUT_DIR = os.path.join(os.path.expanduser('~'), '04_image_translation_data/kidneytissue')\n",
    "# INPUT_DIR = '/mnt/efs/woods_hole/04_image_translation_data/kidneytissue'\n",
    "\n",
    "# And specify where we want to store our preprocessed data\n",
    "PREPROC_OUTPUT_DIR = os.path.join(os.path.expanduser('~'), '04_image_translation_data/tile_small_temp')\n",
    "# PREPROC_OUTPUT_DIR = os.path.join(os.path.expanduser('~'), '04_image_translation_data/tmp/tile_small')\n",
    "\n",
    "# This is where the model weights and related variables will be stored\n",
    "MODEL_DIR = os.path.join(os.path.expanduser('~'), '04_image_translation_data/translation_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BACK TO [TABLE OF CONTENTS](#toc)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Explore input and target images <a class=\"anchor\" id=\"explore\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this exercise is to translate 2D slices of phase or retardance image into 2D slices of fluorescene images of F-actin and DNA. Let's start by looking at some example images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Image \"/home/christian.foley/04_image_translation_data/kidneytissue/img_568_t000_p003_z010.tif\" cannot be found.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-177dab426ab4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0minput_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mINPUT_DIR\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'img_phase_t000_p003_z010.tif'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# the phase and polarization were imaged using 530nm wavelength.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mactin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mim_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactin_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;31m# This will clip the top and bottom 1% of intensitites\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mactin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnorm_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhist_clipping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m99\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/virtual_staining/microDL/micro_dl/utils/image_utils.py\u001b[0m in \u001b[0;36mread_image\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIMREAD_ANYDEPTH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Image \"{}\" cannot be found.'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Image \"/home/christian.foley/04_image_translation_data/kidneytissue/img_568_t000_p003_z010.tif\" cannot be found."
     ]
    }
   ],
   "source": [
    "# import the libaries again if the plots don't display properly\n",
    "actin_path = os.path.join(INPUT_DIR, 'img_568_t000_p003_z010.tif') #F-actin was imaged with Alexa Fluor 568 using 468nm excitation wavelength.\n",
    "dna_path=os.path.join(INPUT_DIR, 'img_405_t000_p003_z010.tif') # DNA was imaged with Hoechst using 405nm excitation wavelength.\n",
    "input_path=os.path.join(INPUT_DIR,'img_phase_t000_p003_z010.tif') # the phase and polarization were imaged using 530nm wavelength.\n",
    "\n",
    "actin = im_utils.read_image(actin_path)\n",
    "# This will clip the top and bottom 1% of intensitites\n",
    "actin = norm_utils.hist_clipping(actin, 1, 99)\n",
    "dna = im_utils.read_image(dna_path)\n",
    "dna = norm_utils.hist_clipping(dna, 1, 99)\n",
    "im_input = im_utils.read_image(input_path) \n",
    "im_input = norm_utils.hist_clipping(im_input, 0.8, 99.5) \n",
    "fig, ax = plt.subplots(1, 3)\n",
    "fig.set_size_inches(20, 15)\n",
    "ax = ax.flatten()\n",
    "ax[0].imshow(im_input, cmap='gray')\n",
    "ax[0].set_title('Input phase',fontsize=20)\n",
    "ax[1].imshow(actin, cmap='gray')\n",
    "ax[1].set_title('Target F-actin',fontsize=20)\n",
    "ax[2].imshow(dna, cmap='gray')\n",
    "ax[2].set_title('Target DNA',fontsize=20)\n",
    "for a in ax: a.axis('off')\n",
    "\n",
    "plt.show()\n",
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BACK TO [TABLE OF CONTENTS](#toc)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Metadata <a class=\"anchor\" id=\"metadata\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "microDL uses CSV file to track the metadata of the images. We'll generate the metadata and take a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_utils.frames_meta_generator(INPUT_DIR)\n",
    "meta_utils.ints_meta_generator(INPUT_DIR,\n",
    "                               num_workers=8,\n",
    "                               )\n",
    "frames_meta = pd.read_csv(os.path.join(INPUT_DIR, 'frames_meta.csv'), index_col=0)\n",
    "frames_meta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each channel in our data is assgined with a unique channel ID. We'll reference the channels by their IDs in the config files. Let's check the channel IDs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chan_id_df = frames_meta[['channel_idx', 'channel_name']].drop_duplicates()\n",
    "chan_id_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BACK TO [TABLE OF CONTENTS](#toc)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Pilot: 2D image translation (slice$\\rightarrow$slice) on small dataset <a class=\"anchor\" id=\"pilot\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing <a class=\"anchor\" id=\"preproc\"></a>\n",
    "\n",
    "We will  be using three main modules of microDL for image translation: preprocessing, training and inference (see imports above).\n",
    "\n",
    "The first step is the preprocessing. We have some options like resizing, flatfield correction, creating masks, and tiling. The data we're working with is already background corrected so we can safely skip that part. We also don't need to resize the images.\n",
    "\n",
    "We would however like to create masks based on our target data. The reason for that is that we would like to avoid training on background only, that is empty or near empty tiles. A threshold we found reasonable is to make sure that our tiles contain 25% signal or more.\n",
    "\n",
    "We would also like to tile our images. The original image size (2048 x 2048 pixels) is too large to be able to fit into memory and similar structures reappear across the images. So we'd be much better off splitting the images into smalle pieces (tiles). A design consideration for convolution neural networks is the [receptive field](https://medium.com/mlreview/a-guide-to-receptive-field-arithmetic-for-convolutional-neural-networks-e0f514068807), which is the region of the input space a given feature is looking at. Given the size of our structures of interest and our network architecture, we use 256 x 256 pixel tiles. Also, training on smaller tiles allow us to use bigger batch size so the training converges faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A. Masks\n",
    "\n",
    "We would like to generate binary masks based on our target channel. Let's for now assume our target channel is channel index = 2. In microDL we have two types of global binary thresholding methods builtin, [Otsu](https://en.wikipedia.org/wiki/Otsu%27s_method) and [unimodal](https://users.cs.cf.ac.uk/Paul.Rosin/resources/papers/unimodal2.pdf) (or Rosin) thresholding.\n",
    "Let's load an image, generate masks and plot them side by side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load image\n",
    "im_path = os.path.join(INPUT_DIR, 'img_568_t000_p003_z010.tif')\n",
    "im = im_utils.read_image(im_path)\n",
    "# Clip top and bottom 1% of histogram for better visualization\n",
    "im = norm_utils.hist_clipping(im, 1, 99)\n",
    "\n",
    "mask_otsu = mask_utils.create_otsu_mask(im)\n",
    "mask_rosin = mask_utils.create_unimodal_mask(im)\n",
    "\n",
    "fig, ax = plt.subplots(1, 3)\n",
    "fig.set_size_inches(20, 15)\n",
    "ax = ax.flatten()\n",
    "ax[0].imshow(im, cmap='gray')\n",
    "ax[0].set_title('Fluorescecne',fontsize=20)\n",
    "ax[1].imshow(mask_otsu, cmap='gray')\n",
    "ax[1].set_title('Otsu thresholding',fontsize=20)\n",
    "ax[2].imshow(mask_rosin, cmap='gray')\n",
    "ax[2].set_title('unimodal thresholding',fontsize=20)\n",
    "for a in ax: a.axis('off')\n",
    "\n",
    "plt.show()\n",
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the Otsu threshold captures only the very bright structures and misses dim structures. Rosin thresholding does a better job at these dim structures, so we'll be using Rosin thresholding for the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B. configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually we run microDL through command line interfaces (CLIs), which is to say we just input commands in the terminal. For each module we specify the path to a config file as a command line argument. Since we're using Jupyter Notebook for this tutorial we will instead load the preprocessing config so we can take a look at it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = os.path.join(\n",
    "    module_path,\n",
    "    'micro_dl/config_preprocess.yml',\n",
    ")\n",
    "preproc_config = aux_utils.read_config(config_path)\n",
    "pp.pprint(preproc_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "There are some things we need to change around here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're not doing resizing so let's remove that key\n",
    "if 'resize' in preproc_config:\n",
    "    preproc_config.pop('resize')\n",
    "# We're not doing flatfield correction either\n",
    "if 'flat_field' in preproc_config:\n",
    "    preproc_config.pop('flat_field')\n",
    "    \n",
    "# We need to change input_dir to point to where our image data is located\n",
    "preproc_config['input_dir'] = INPUT_DIR\n",
    "\n",
    "# And where we want to store our preprocessed data\n",
    "preproc_config['output_dir'] = PREPROC_OUTPUT_DIR\n",
    "\n",
    "# Set which channels we want to preprocess and if they should be normalized\n",
    "preproc_config['channel_ids'] = [0, 1, 2, 3]\n",
    "preproc_config['normalize']['normalize_channels'] = [True, True, True, True]\n",
    "\n",
    "# Set the channels used for generating masks\n",
    "preproc_config['masks']['channels'] = 1\n",
    "\n",
    "# Switch to unimodal (Rosin) thresholding\n",
    "preproc_config['masks']['mask_type'] = 'unimodal'\n",
    "\n",
    "# Set the number of workers to the number of available cores\n",
    "preproc_config['num_workers'] = 8\n",
    "\n",
    "# Let's look again\n",
    "pp.pprint(preproc_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### C. Preprocess data\n",
    "Now it's time to run the preprocessing: the runtime should be aroud 5 minute. \n",
    "If this step is taking too long. You can use the previously processed data by setting:\n",
    "\n",
    "`INPUT_DIR = '/mnt/efs/woods_hole/04_image_translation_data/kidneytissue'`\n",
    "\n",
    "`PREPROC_OUTPUT_DIR = os.path.join(os.path.expanduser('~'), '04_image_translation_data/tmp/tile_small')`\n",
    "\n",
    "If you get an error about \"Don't specify a mask_dir\", try reloading the config by running the last two blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc_config, runtime = preprocess.pre_process(preproc_config)\n",
    "print(\"Preprocessing took {} seconds\".format(runtime))\n",
    "# Save the final config and run time\n",
    "preprocess.save_config(preproc_config, runtime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "You can navigate to your output_dir and take a look at what was generated. You will find a mask_dir containing masks, a tile_dir containing tiles and JSON file containing the all the information that was used to generated the preprocessed data. Let's take a look at a few tiles. Change `tile_dir` to `os.path.join(os.path.expanduser('~'), '04_image_translation_data/tmp/tile_small/tiles_256-256_step_128-128' )` if preprocessing didn't finish properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tile_dir = preproc_config['tile']['tile_dir']\n",
    "# tile_dir =os.path.join(os.path.expanduser('~'), '04_image_translation_data/tmp/tile_small/tiles_256-256_step_128-128' )\n",
    "print(tile_dir)\n",
    "frames_meta = pd.read_csv(os.path.join(tile_dir, 'frames_meta.csv'))\n",
    "# Randomly select 40 tiles and plot them\n",
    "subset = np.random.choice(frames_meta.shape[0], 40, replace=False)\n",
    "fig, ax = plt.subplots(4, 10)\n",
    "fig.set_size_inches(20, 10)\n",
    "ax = ax.flatten()\n",
    "for i, axi in enumerate(ax):\n",
    "    im = im_utils.read_image(os.path.join(tile_dir, frames_meta.loc[subset[i], 'file_name']))\n",
    "    print(frames_meta.loc[subset[i], 'file_name'], 'im shape', im.shape)\n",
    "    axi.imshow(np.squeeze(im), cmap='gray'); axi.axis('off')\n",
    "plt.show()\n",
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BACK TO [TABLE OF CONTENTS](#toc)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training  <a class=\"anchor\" id=\"train\"></a>\n",
    "Now that we've preprocessed our data we're ready to train. During this exercise, we will use phase or retardance image to predict two different fluorescent channels, actin and nuclei.\n",
    "In our dataset, the channel names and indices are the following:\n",
    "\n",
    "* Retardance: channel name Retardance, index 2\n",
    "* Phase: channel name phase, index 3\n",
    "* Actin:channel name 568, index 1\n",
    "* Nuclei: channel name 405, index 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A. configuration\n",
    "\n",
    "We specify the network architecture and training parameters using another config file. Let's load a base 2D training config file and take a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = os.path.join(\n",
    "    module_path,\n",
    "    'micro_dl/config_train.yml',\n",
    ")\n",
    "train_config = aux_utils.read_config(config_path)\n",
    "pp.pprint(train_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a lot more options here, as you can see. The first things to check for when doing image translation is that the final activation is linear. Unlike for binary segmentation when we want to use a sigmoid to move values towards either zero or one, we would here like to do a regression and not apply any transform to our output signal.\n",
    "\n",
    "There are some variables that it would be interesting for you to explore while training:\n",
    "\n",
    "##### Loss\n",
    "The second thing is to choose a loss function that makes sense. Common choices for regression are the mean squared error (MSE) and the mean absolute error (MAE) between the target image y and the estimated image y':\n",
    "\\begin{equation*}\n",
    "MSE = \\sum_{p} (y_p - y_p')^2,\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "MAE = \\sum_{p} |y_p - y_p'|\n",
    "\\end{equation*}\n",
    "for each pixel index p.\n",
    "\n",
    "You can try both by changing train_config['trainer']['loss']. The names in microDL for losses are mse_loss and mae_loss, respectively. You can also try other custom losses by using the loss function names [here](https://github.com/czbiohub/microDL/blob/master/micro_dl/train/losses.py), or any standard [Keras loss function](https://keras.io/losses/) by specifying the loss function name defined by Keras. \n",
    "\n",
    "##### Optimizer\n",
    "A good default choice of optimizer is Adam. You can read more about different deep learning optimizers [here](http://ruder.io/optimizing-gradient-descent/), and you can change the optimizer you'd like to use in your training by changing the variable train_config['trainer']['optimizer']['name'] to any of the Keras optimizers listed [here](https://keras.io/optimizers/).\n",
    "\n",
    "##### Learning Rate\n",
    "If the learning rate is too small your training might take a very long time to converge, and if it's too big it might not converge at all. It's worth trying some different values and see what happens with convergence.\n",
    "\n",
    "##### Dropout\n",
    "Since we're working with a very small dataset in exploratory training, chances are that your network will overfit to your training data. It's worth exploring train_config['network']['dropout'] and to see if increasing those variables can reduce overfitting.\n",
    "\n",
    "##### Number of filters\n",
    "The number of filters in each layer of the model controls the model capacity. This parameter is train_config['network']['num_filters_per_block']. Too large model capacity can lead to overfitting and not necesssarily better results.\n",
    "\n",
    "##### Augmentation\n",
    "The data is flipped and rotated randomly to diversify the training set and mitigate overfitting.\n",
    "\n",
    "##### Other?\n",
    "If you have extra time or are curious about the other variables, feel free to explore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our training config, we need to make some adjustments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the data directory to the directory we generated during preprocessing\n",
    "train_config['dataset']['data_dir'] = os.path.join(PREPROC_OUTPUT_DIR, 'tiles_256-256_step_128-128')\n",
    "\n",
    "# We also need to specify where we want to store our model and all related data generated by training\n",
    "# This directory will be created if it doesn't already exist\n",
    "train_config['trainer']['model_dir'] = MODEL_DIR\n",
    "\n",
    "# Set maximum number of epochs to 6 so we can explore difference parameters quickly\n",
    "train_config['trainer']['max_epochs'] = 6\n",
    "train_config['trainer']['batch_size'] = 64\n",
    "# Predict actin (channel 1) from phase (channel 3)\n",
    "train_config['dataset']['input_channels'] = [3]\n",
    "train_config['dataset']['target_channels'] = [1]\n",
    "\n",
    "# Enable data augmentation.\n",
    "train_config['dataset']['augmentation'] = True\n",
    "# Use all training data each epoch\n",
    "if 'train_fraction' in train_config['dataset']:\n",
    "    train_config['dataset'].pop('train_fraction')\n",
    "# Set number of filters\n",
    "train_config['network']['num_filters_per_block'] = [16, 32, 48, 64, 80]\n",
    "# Select L1 loss\n",
    "train_config['trainer']['loss'] = 'mae_loss'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### B. Time to train. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, specify the gpu that you want to run training on, or \"None\" to select the gpu with most memory available\n",
    "gpu_id = 0\n",
    "gpu_id, gpu_mem_frac = train_utils.select_gpu(gpu_ids=gpu_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try restarting the notebook kernel if the gpu memory is occupied and you run into errors about \"can't creat training session\".\n",
    "Training 6 epochs should take no more than 5 minutes if you're on a GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.run_action(\n",
    "    action='train',\n",
    "    config=train_config,\n",
    "    gpu_ids=gpu_id,\n",
    "    gpu_mem_frac=gpu_mem_frac,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've completed our first training. Let's take a look at what happened during training by opening a history log file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = pd.read_csv(os.path.join(MODEL_DIR, 'history.csv'))\n",
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training vs. validation loss\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(12, 9)\n",
    "ax.plot(history['epoch'], history['loss'], 'r')\n",
    "ax.plot(history['epoch'], history['val_loss'], 'b')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss')\n",
    "plt.legend(['train', 'validaiton'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Does it look like the model is overfitting? How can you tell?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BACK TO [TABLE OF CONTENTS](#toc)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Predictions on test set <a class=\"anchor\" id=\"predict\"></a>\n",
    "\n",
    "We'd also like to see how well the model performs predictions. For that we will have to run inference on our test dataset. We will run model inference on the full size 2048 X 2048 image instead of on tiles in training. Why can we run the model inference on different input size? And what are the benefits of doing that?   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A. configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = os.path.join(\n",
    "    module_path,\n",
    "    'micro_dl/config_inference.yml',\n",
    ")\n",
    "inf_config = aux_utils.read_config(config_path)\n",
    "pp.pprint(inf_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inf_config['image_dir'] = INPUT_DIR \n",
    "inf_config['preprocess_dir'] = PREPROC_OUTPUT_DIR\n",
    "inf_config['model_dir'] = MODEL_DIR\n",
    "inf_config['dataset']['input_channels'] = [3]\n",
    "inf_config['dataset']['target_channels'] = [1]\n",
    "inf_config['metrics']['metrics_orientations'] = ['xy']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B. Run prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_inst = image_inf.ImagePredictor(\n",
    "                train_config=train_config,\n",
    "                inference_config=inf_config,\n",
    "                preprocess_config=preproc_config,\n",
    "                gpu_id=gpu_id,\n",
    "                gpu_mem_frac=gpu_mem_frac,\n",
    "            )\n",
    "inference_inst.run_prediction()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There should be a new subdirectory created in the model directory with the predictions and the metrics. Use glob to see what files were generated during inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dir = os.path.join(MODEL_DIR, 'predictions')\n",
    "glob.glob(os.path.join(pred_dir, '*'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot an example of input, target and prediction side by side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3)\n",
    "fig.set_size_inches(20, 15)\n",
    "ax = ax.flatten()\n",
    "for a in ax: a.axis('off')\n",
    "\n",
    "im_path = os.path.join(INPUT_DIR, 'img_phase_t000_p011_z013.tif')\n",
    "im = im_utils.read_image(im_path)\n",
    "im = norm_utils.hist_clipping(im, 1, 99)\n",
    "ax[0].imshow(im, cmap='gray'); ax[0].set_title('Input: Phase', fontsize=20)\n",
    "im_path = os.path.join(INPUT_DIR, 'img_568_t000_p011_z013.tif')\n",
    "im = im_utils.read_image(im_path)\n",
    "im = norm_utils.hist_clipping(im, 1, 99)\n",
    "ax[1].imshow(im, cmap='gray'); ax[1].set_title('Target 1: Actin', fontsize=20)\n",
    "im_path = os.path.join(pred_dir, 'img_pred_t000_p011_z013.tif')\n",
    "im = im_utils.read_image(im_path)\n",
    "im = norm_utils.hist_clipping(im, 1, 99)\n",
    "ax[2].imshow(im, cmap='gray'); ax[2].set_title('Prediction of Actin', fontsize=20)\n",
    "plt.show()\n",
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prediction is much blurrier than the target. One reason you can't exactly mimic the F-actin target, because the input channel carries only part of the information about the structure and the random noise cannot be learned.\n",
    "\n",
    "Another reason for blurry prediction is that loss functions like MSE or MAE tend to generate blurrier prediction because these loss functions generate an \"average\" prediction when there are multiple possible predictions.  \n",
    "\n",
    "Also, we've here used a very limited amount of data. To get higher correlation we will need to include much more data and run training overnight.\n",
    "\n",
    "Speaking of correlation, let's open the inference meta file and inspect the metrics comparing predictions and targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_meta = pd.read_csv(os.path.join(pred_dir, 'metrics_xy.csv'))\n",
    "metrics_meta.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the median correlation of all rows in the inference csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Median Pearson correlation: {:.2f}\".format(metrics_meta['corr'].median()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BACK TO [TABLE OF CONTENTS](#toc)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model tuning <a class=\"anchor\" id=\"tuning\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time for you to experiment. You can try modeling a different channel (use retardance channel to predict F-actin) or play around with different settings in the train_config and rerun the training. What do you think will help improve the model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BACK TO [TABLE OF CONTENTS](#toc)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Bonus: Training on larger dataset (slice$\\rightarrow$slice)<a class=\"anchor\" id=\"bonus\"></a>\n",
    "\n",
    "Hopefully above exploration has led you to set of parameters to predict F-actin and nuclei with decent accuracy. You  can now set up a model to train on larger dataset (30 FOVs) and evaluate if model accuracy improves as a result when you comeback. \n",
    "\n",
    "We will need change the position ids in the pre-process config to have 30 FOVs and re-run preprocessing. The position ids of 30 FOVs are:\n",
    "\n",
    "[3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]\n",
    "\n",
    "You can pick training/model parameters based on your parameter exploration from Session 1. A few tips for picking training parameters for overnight training:\n",
    "- **Make sure you write preprocessed data and model trained on this large set to new folders.**\n",
    "- Increase maximum number of epochs and early stopping patience to at least 200 and 10 so the training will run longer\n",
    "- Increase the number of filters in the model to increase the model capacity. You might need to use smaller batch size so the model can fit into the GPU memory You might want to add some dropout as well to avoid overfitting if you increase the number of filters \n",
    "- Use lower learning rate. We used higher learning rate to make training converge faster\n",
    "- Compare the mean and standard deviation of test metrics.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BACK TO [TABLE OF CONTENTS](#toc)**"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python [conda env:.conda-pt_tf_cross]",
   "language": "python",
   "name": "conda-env-.conda-pt_tf_cross-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "toc-showcode": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
