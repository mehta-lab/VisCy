{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Translation\n",
    "\n",
    "This will provide you an introduction image translation formulated as a regression problem using a residual U-Net model.\n",
    "Hopefully you will be acqainted with the U-Net model by now. At its core, it's an autoencoder, consisting of an encoder (downsampling) part and a decoder (upsampling) part. The U-Net model has become immensely popular in many image analysis tasks, and one of its main strengths are the skip connections between the encoder and decoder layers which allows the model to capture details at and near the input level resolution. If you've seen U-Nets being used for image segmentation previously, you only need minor adjustments to adapt it to image regression, by changing the loss function and the final activation layer (more on that later).\n",
    "\n",
    "Below is an overview of the 2D U-Net model architecture as in the original paper (figure courtesy of [Uni Freiburg](https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/)). We'll be using a very similar architecture but with same padding, which means we pad the input image so the input image and the output image will have the same shape after covolution. We also have a short skip connection within each block in addtion to the long skip connections in the original U-Net.  \n",
    "\n",
    "![U-Net architecture](u-net-architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Import libraries and set paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with some imports. We'll be using the open source U-Net repository [microDL](https://github.com/czbiohub/microDL) , so we'll import that along with some plotting and other utility packages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import importlib\n",
    "import os\n",
    "import pandas as pd\n",
    "import pprint\n",
    "import sys\n",
    "\n",
    "import micro_dl.cli.image_inference as inference\n",
    "import micro_dl.cli.train_script as train\n",
    "import micro_dl.cli.preprocess_script as preprocess\n",
    "import micro_dl.utils.aux_utils as aux_utils\n",
    "import micro_dl.utils.image_utils as im_utils\n",
    "import micro_dl.utils.masks as mask_utils\n",
    "import micro_dl.utils.normalize as norm_utils\n",
    "import micro_dl.utils.train_utils as train_utils\n",
    "\n",
    "# Add module path to sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "print(module_path)\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "# Setup pretty print\n",
    "pp = pprint.PrettyPrinter(indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define paths where input images are stored and where processed data and model will be saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the directory where our imaging data is downloaded\n",
    "INPUT_DIR = '/home/paperspace/Data/MBL_DL_image_translation/data/'\n",
    "# And specify where we want to store our preprocessed data\n",
    "PREPROC_OUTPUT_DIR = '/home/paperspace/Data/translation_preprocessed_data/'\n",
    "# This is where the model weights and related variables will be stored\n",
    "MODEL_DIR = '/home/paperspace/Data/translation_model_adam_mae'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this exercise is to translate 2D slices of brightfield image into 2D slices of fluorescene images of F-actin and DNA. Let's start by looking at some example images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actin_path = os.path.join(INPUT_DIR, 'img_568_t000_p003_z010.tif')\n",
    "dna_path=os.path.join(INPUT_DIR, 'img_405_t000_p003_z010.tif')\n",
    "bf_path=os.path.join(INPUT_DIR,'img_Transmission_t000_p003_z010.tif')\n",
    "actin = im_utils.read_image(actin_path)\n",
    "# This will clip the top and bottom 1% of intensitites\n",
    "actin = norm_utils.hist_clipping(actin, 1, 99)\n",
    "dna = im_utils.read_image(dna_path)\n",
    "dna = norm_utils.hist_clipping(dna, 1, 99)\n",
    "brightfield = im_utils.read_image(bf_path) \n",
    "brightfield = norm_utils.hist_clipping(brightfield, 0.8, 99.5) \n",
    "\n",
    "plt.subplot(131); plt.imshow(brightfield, cmap='gray'); plt.axis('off')\n",
    "plt.title('Input brightfield',fontSize=20)\n",
    "plt.subplot(132); plt.imshow(actin, cmap='gray'); plt.axis('off')\n",
    "plt.title('Target F-actin',fontSize=20)\n",
    "plt.subplot(133); plt.imshow(dna, cmap='gray'); plt.axis('off')\n",
    "plt.title('Target DNA',fontSize=20)\n",
    "plt.rcParams['figure.figsize'] = [20, 15]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 1: image translation (slice$\\rightarrow$slice) on small dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preprocessing\n",
    "\n",
    "We will  be using three main modules of microDL for image translation: preprocessing, training and inference (see imports above).\n",
    "\n",
    "The first step is the preprocessing. We have some options like resizing, flatfield correction, creating masks, and tiling. The data we're working with is already background corrected so we can safely skip that part. We also don't need to resize the images.\n",
    "\n",
    "We would however like to create masks based on our target data. The reason for that is that we would like to avoid training on background only, that is empty or near empty tiles. A threshold we found reasonable is to make sure that our tiles contain 25% signal or more.\n",
    "\n",
    "We would also like to tile our images. The original image size (2048 x 2048 pixels) is too large to be able to fit into memory and similar structures reappear across the images. So we'd be much better off splitting the images into smalle pieces (tiles). A design consideration for convolution neural networks is the [receptive field](https://medium.com/mlreview/a-guide-to-receptive-field-arithmetic-for-convolutional-neural-networks-e0f514068807), which is the region of the input space a given feature is looking at. Given the size of our structures of interest and our network architecture, we use 256 x 256 pixel tiles. Also, training on smaller tiles allow us to use bigger batch size so the training converges faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Masks\n",
    "\n",
    "We would like to generate binary masks based on our target channel. Let's for now assume our target channel is channel index = 2. In microDL we have two types of global binary thresholding methods builtin, [Otsu](https://en.wikipedia.org/wiki/Otsu%27s_method) and [unimodal](https://users.cs.cf.ac.uk/Paul.Rosin/resources/papers/unimodal2.pdf) (or Rosin) thresholding.\n",
    "Let's load an image, generate masks and plot them side by side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load image\n",
    "im_path = os.path.join(INPUT_DIR, 'img_568_t000_p003_z010.tif')\n",
    "im = im_utils.read_image(im_path)\n",
    "# Clip top and bottom 1% of histogram for better visualization\n",
    "im = norm_utils.hist_clipping(im, 1, 99)\n",
    "\n",
    "mask_otsu = mask_utils.create_otsu_mask(im)\n",
    "mask_rosin = mask_utils.create_unimodal_mask(im)\n",
    "\n",
    "plt.subplot(131); plt.imshow(im, cmap='gray'); plt.axis('off')\n",
    "plt.subplot(132); plt.imshow(mask_otsu, cmap='gray'); plt.axis('off')\n",
    "plt.subplot(133); plt.imshow(mask_rosin, cmap='gray'); plt.axis('off')\n",
    "plt.rcParams['figure.figsize'] = [30, 20]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the Otsu threshold captures only the very bright structures and misses dim structures. Rosin thresholding does a better job at these dim structures, so we'll be using Rosin thresholding for the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B.  Preprocessing configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually we run microDL through command line interfaces (CLIs), which is to say we just input commands in the terminal. For each module we specify the path to a config file as a command line argument. Since we're using Jupyter Notebook for this tutorial we will instead load, the preprocessing config so we can take a look at it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = os.path.join(\n",
    "    module_path,\n",
    "    'micro_dl/config_preprocess.yml',\n",
    ")\n",
    "preproc_config = aux_utils.read_config(config_path)\n",
    "pp.pprint(preproc_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some things we need to change around here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're not doing resizing so let's remove that key\n",
    "if 'resize' in preproc_config:\n",
    "    preproc_config.pop('resize')\n",
    "# We're not doing flatfield correction either\n",
    "if 'flat_field' in preproc_config:\n",
    "    preproc_config.pop('flat_field')\n",
    "    \n",
    "# We need to change input_dir to point to where our image data is located\n",
    "preproc_config['input_dir'] = INPUT_DIR\n",
    "# And where we want to store our preprocessed data\n",
    "preproc_config['output_dir'] = PREPROC_OUTPUT_DIR\n",
    "\n",
    "# Switch to unimodal (Rosin) thresholding\n",
    "preproc_config['masks']['mask_type'] = 'unimodal'\n",
    "\n",
    "# Our VMs have 8 cores so adjust the number of workers\n",
    "preproc_config['num_workers'] = 8\n",
    "\n",
    "# Let's look again\n",
    "pp.pprint(preproc_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's break apart our config and also set some other parameters\n",
    "# Typically microscopy images are indexed along channels, time, FOV (position) and slice (z)\n",
    "# index of -1 mean include all indices\n",
    "slice_ids = -1\n",
    "if 'slice_ids' in preproc_config:\n",
    "    slice_ids = preproc_config['slice_ids']\n",
    "time_ids = -1\n",
    "if 'time_ids' in preproc_config:\n",
    "    time_ids = preproc_config['time_ids']\n",
    "pos_ids = -1\n",
    "if 'pos_ids' in preproc_config:\n",
    "    pos_ids = preproc_config['pos_ids']\n",
    "channel_ids = -1\n",
    "if 'channel_ids' in preproc_config:\n",
    "    channel_ids = preproc_config['channel_ids']\n",
    "# The images are uniform in size\n",
    "uniform_struct = True\n",
    "# How many chars an index should be encoded as\n",
    "int2str_len = 3\n",
    "# Number of processes to use\n",
    "num_workers = 4\n",
    "if 'num_workers' in preproc_config:\n",
    "    num_workers = preproc_config['num_workers']\n",
    "\n",
    "base_config = {'input_dir': INPUT_DIR,\n",
    "               'output_dir': PREPROC_OUTPUT_DIR,\n",
    "               'slice_ids': slice_ids,\n",
    "               'time_ids': time_ids,\n",
    "               'pos_ids': pos_ids,\n",
    "               'channel_ids': channel_ids,\n",
    "               'uniform_struct': uniform_struct,\n",
    "               'int2strlen': int2str_len,\n",
    "               'num_workers': num_workers}\n",
    "\n",
    "# Let's look at the base_config we just generated\n",
    "pp.pprint(base_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Preprocess data\n",
    "Now it's time to do run the preprocessing: the runtime should be less than a minute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc_config, runtime = preprocess.pre_process(preproc_config, base_config)\n",
    "print(\"Preprocessing took {} seconds\".format(runtime))\n",
    "# Save the final config and run time\n",
    "preprocess.save_config(preproc_config, runtime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can navigate to your output_dir and take a look at what was generated. You will find a mask_dir containing masks, a tile_dir containing tiles and JSON file containing the all the information that was used to generated the preprocessed data. Let's take a look at a few tiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tile_dir = preproc_config['tile']['tile_dir']\n",
    "print(tile_dir)\n",
    "frames_meta = pd.read_csv(os.path.join(tile_dir, 'frames_meta.csv'))\n",
    "# Randomly select 5 tiles and plot them\n",
    "subset = np.random.choice(frames_meta.shape[0], 5, replace=False)\n",
    "print(subset)\n",
    "for i in range(5):\n",
    "    im = im_utils.read_image(os.path.join(tile_dir, frames_meta.loc[subset[i], 'file_name']))\n",
    "    print(frames_meta.loc[subset[i], 'file_name'], 'im shape', im.shape)\n",
    "    plt.subplot(1, 5, i + 1); plt.imshow(np.squeeze(im), cmap='gray'); plt.axis('off')\n",
    "plt.rcParams['figure.figsize'] = [15, 5]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  2. Training \n",
    "Now that we've preprocessed our data we're ready to train. During this exercise, we will use brightfield image to predict two different fluorescent channels, actin and nuclei.\n",
    "In our dataset, the channel names and indices are the following:\n",
    "\n",
    "* Brightfield: channel name Transmission, index 2\n",
    "* Actin:channel name 568, index 1\n",
    "* Nuclei: channel name 405, index 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.  Training configuration\n",
    "\n",
    "We specify the network architecture and training parameters using another config file. Let's load a base 2D training config file and take a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = os.path.join(\n",
    "    module_path,\n",
    "    'micro_dl/config.yml',\n",
    ")\n",
    "train_config = aux_utils.read_config(config_path)\n",
    "pp.pprint(train_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a lot more options here, as you can see. The first things to check for when doing image translation is that the final activation is linear. Unlike for binary segmentation when we want to use a sigmoid to move values towards either zero or one, we would here like to do a regression and not apply any transform to our output signal.\n",
    "\n",
    "There are some variables that it would be interesting for you to explore while training:\n",
    "\n",
    "#### Loss\n",
    "The second thing is to choose a loss function that makes sense. Common choices for regression are the mean squared error (MSE) and the mean absolute error (MAE) between the target image y and the estimated image y':\n",
    "\\begin{equation*}\n",
    "MSE = \\sum_{p} (y_p - y_p')^2,\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "MAE = \\sum_{p} |y_p - y_p'|\n",
    "\\end{equation*}\n",
    "for each pixel index p.\n",
    "\n",
    "You can try both by changing train_config['trainer']['loss']. The names in microDL for losses are mse_loss and mae_loss, respectively. You can also try other custom losses by using the loss function names [here](https://github.com/czbiohub/microDL/blob/master/micro_dl/train/losses.py), or any standard [Keras loss function](https://keras.io/losses/) by specifying the loss function name defined by Keras. \n",
    "\n",
    "#### Optimizer\n",
    "A good default choice of optimizer is Adam. You can read more about different deep learning optimizers [here](http://ruder.io/optimizing-gradient-descent/), and you can change the optimizer you'd like to use in your training by changing the variable train_config['trainer']['optimizer']['name'] to any of the Keras optimizers listed [here](https://keras.io/optimizers/).\n",
    "\n",
    "#### Learning Rate\n",
    "If the learning rate is too small your training might take a very long time to converge, and if it's too big it might not converge at all. It's worth trying some different values and see what happens with convergence.\n",
    "\n",
    "#### Dropout\n",
    "Since we're working with a very small dataset in exploratory training, chances are that your network will overfit to your training data. It's worth exploring train_config['network']['dropout'] and to see if increasing those variables can reduce overfitting.\n",
    "\n",
    "#### Numer of filters\n",
    "The number of filters in each layer of the model controls the model capacity. This parameter is train_config['network']['num_filters_per_block']. Too large model capacity can lead to overfitting and not necesssarily better results.\n",
    "\n",
    "#### Augmentation\n",
    "The data is flipped and rotated randomly to diversify the training set and mitigate overfitting.\n",
    "\n",
    "#### Other?\n",
    "If you have extra time or are curious about the other variables, feel free to explore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our training config, we need to make some adjustments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the data directory to the directory we generated during preprocessing\n",
    "train_config['dataset']['data_dir'] = PREPROC_OUTPUT_DIR\n",
    "\n",
    "# We also need to specify where we want to store our model and all related data generated by training\n",
    "# This directory will be created if it doesn't already exist\n",
    "train_config['trainer']['model_dir'] = MODEL_DIR\n",
    "\n",
    "# Set maximum number of epochs to 6 so we can explore difference parameters quickly\n",
    "train_config['trainer']['max_epochs'] = 6\n",
    "\n",
    "# Predict actin (channel 1) from brightfield (channel 2)\n",
    "train_config['dataset']['input_channels'] = [2]\n",
    "train_config['dataset']['target_channels'] = [1]\n",
    "\n",
    "# Enable data augmentation.\n",
    "train_config['dataset']['augmentation'] = True\n",
    "# Use all training data each epoch\n",
    "if 'train_fraction' in train_config['dataset']:\n",
    "    train_config['dataset'].pop('train_fraction')\n",
    "# Set number of filters\n",
    "train_config['network']['num_filters_per_block'] = [16, 32, 48, 64, 80]\n",
    "# Select L1 loss\n",
    "train_config['trainer']['loss'] = 'mae_loss'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to train. First, let see what GPU resources are available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_id, gpu_mem_frac = train_utils.select_gpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try restarting the notebook kernel if you run into errors about \"can't creat training session\".\n",
    "Training 6 epochs should take no more than 5 minutes if you're on a GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.run_action(\n",
    "    action='train',\n",
    "    config=train_config,\n",
    "    gpu_ids=gpu_id,\n",
    "    gpu_mem_frac=gpu_mem_frac,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've completed our first training. Let's take a look at what happened during training by opening a history log file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = pd.read_csv(os.path.join(MODEL_DIR, 'history.csv'))\n",
    "history.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training vs. validation loss\n",
    "plt.plot(history.epoch, history.loss, 'r')\n",
    "plt.plot(history.epoch, history.val_loss, 'b')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.ylim(top=.9)\n",
    "plt.legend(['train', 'validaiton'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does it look like the model is overfitting? How can you tell?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Predictions on test set\n",
    "\n",
    "We'd also like to see how well the model performs predictions. For that we will have to run inference on our test dataset. We will run model inference on the full zie 2048 X 2048 image instead of on tiles in training. Why can we run the model inference on different input size? And what are the benefits of doing that?   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference.run_prediction(\n",
    "    model_dir=MODEL_DIR,\n",
    "    image_dir=INPUT_DIR,\n",
    "    gpu_ids=gpu_id,\n",
    "    gpu_mem_frac=gpu_mem_frac,\n",
    "    metrics=['ssim', 'pearson_corr', 'coeff_determination'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There should be a new subdirectory created the model directory with the predictions and the metrics. Use glob to see what files were generated during inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dir = os.path.join(MODEL_DIR, 'predictions')\n",
    "glob.glob(os.path.join(pred_dir, '*'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot an example of input, target and prediction side by side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_path = os.path.join(INPUT_DIR, 'img_Transmission_t000_p011_z013.tif')\n",
    "im = im_utils.read_image(im_path)\n",
    "im = norm_utils.hist_clipping(im, 1, 99)\n",
    "plt.subplot(131); plt.imshow(im, cmap='gray'); plt.title('Input: Brightfield'); plt.axis('off')\n",
    "im_path = os.path.join(INPUT_DIR, 'img_568_t000_p011_z013.tif')\n",
    "im = im_utils.read_image(im_path)\n",
    "im = norm_utils.hist_clipping(im, 1, 99)\n",
    "plt.subplot(132); plt.imshow(im, cmap='gray'); plt.title('Target 1: Actin'); plt.axis('off')\n",
    "im_path = os.path.join(pred_dir, 'im_c002_z013_t000_p011.tif')\n",
    "im = im_utils.read_image(im_path)\n",
    "im = norm_utils.hist_clipping(im, 1, 99)\n",
    "plt.subplot(133); plt.imshow(im, cmap='gray'); plt.title('Prediction of Actin'); plt.axis('off')\n",
    "plt.rcParams['figure.figsize'] = [15, 10]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prediction is much blurrier than the target. One reason you can't exactly mimic the F-actin target, because the input channel carries only part of the information about the structure and the random noise cannot be learned.\n",
    "\n",
    "Another reason for blurry prediction is that loss functions like MSE or MAE tend to generate blurrier prediction because these loss functions generate an \"average\" prediction when there are multiple possible predictions.  \n",
    "\n",
    "Also, we've here used a very limited amount of data. To get higher correlation we will need to include much more data and run training overnight.\n",
    "\n",
    "Speaking of correlation, let's take open the inference meta file and inspect the metrics comparing predictions and targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_meta = pd.read_csv(os.path.join(pred_dir, 'inference_meta.csv'))\n",
    "inference_meta.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the median correlation of all rows in the inference csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Median Pearson correlation: {:.2f}\".format(inference_meta.pearson_corr.median()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Parameter optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time for you to experiment. You can try modeling a different channel (use brightfield to predict nuclei) or play around with different settings in the train_config and rerun the training. What do you think will help improve the model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Session 2: Training on larger dataset (slice$\\rightarrow$slice)\n",
    "\n",
    "Hopefully above exploration has led you to set of parameters to predict F-actin and nuclei with decent accuracy. You  can now set up a model to train on larger dataset (30 FOVs) and evaluate if model accuracy improves as a result when you comeback. \n",
    "\n",
    "We will need change the position ids in the pre-process config to have 30 FOVs and re-run preprocessing. The position ids of 30 FOVs are:\n",
    "\n",
    "[3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]\n",
    "\n",
    "You can pick training/model parameters based on your parameter exploration from Session 1. A few tips for picking training parameters for overnight training:\n",
    "- **Make sure you write preprocessed data and model trained on this large set to new folders.**\n",
    "- Increase maximum number of epochs and early stopping patience to at least 200 and 10 so the training will run longer\n",
    "- Increase the number of filters in the model to increase the model capacity. You might need to use smaller batch size so the model can fit into the GPU memory You might want to add some dropout as well to avoid overfitting if you increase the number of filters \n",
    "- Use lower learning rate than Session 1. We used higher learning rate in Session 1 to make training converge faster\n",
    "- Compare the mean and standard deviation of test metrics.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
