{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5b78b22",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "# Quick Start: Cytoland\n",
    "\n",
    "**Estimated time to complete:** 15 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7945bca8",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "# Learning Goals\n",
    "\n",
    "* Download the VSCyto2D model and an example dataset containing A549 cell images.\n",
    "* Run VSCyto2D model inference for joint virtual staining of cell nuclei and plasma membrane.\n",
    "* Visualize and compare virtually and experimentally stained cells."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcac596b",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "# Prerequisites\n",
    "Python>=3.10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9102eb78",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "# Introduction\n",
    "\n",
    "## Model\n",
    "\n",
    "The Cytoland virtual staining models are a collection of models (VSCyto2D, VSCyto3D, and VSNeuromast)\n",
    "used to predict cellular landmarks (e.g., nuclei and plasma membranes)\n",
    "from label-free images (e.g. quantitative phase, Zernike phase contrast, and brightfield).\n",
    "This quick-start guide focuses on the VSCyto2D model.\n",
    "\n",
    "VSCyto2D is a 2D UNeXt2 model that has been trained on A549, HEK293T, and BJ-5ta cells.\n",
    "This model enables users to jointly stain cell nuclei and plasma membranes from 2D label-free images\n",
    "that are commonly generated for image-based screens.\n",
    "\n",
    "Alternative models are optimized for different sample types and imaging conditions:\n",
    "\n",
    "* [VSCyto3D](https://public.czbiohub.org/comp.micro/viscy/VS_models/VSCyto3D):\n",
    "3D UNeXt2 model for joint virtual staining of cell nuclei and plasma membrane\n",
    "from high-resolution volumetric images.\n",
    "* [VSNeuromast](https://public.czbiohub.org/comp.micro/viscy/VS_models/VSNeuromast):\n",
    "3D UNeXt2 model for joint virtual staining of nuclei and plasma membrane in zebrafish neuromasts.\n",
    "\n",
    "## Example Dataset\n",
    "\n",
    "The A549 example dataset used in this quick-start guide contains\n",
    "quantitative phase and paired fluorescence images of cell nuclei and plasma membrane.\n",
    "It is stored in OME-Zarr format and can be downloaded from\n",
    "[here](https://public.czbiohub.org/comp.micro/viscy/VS_datasets/VSCyto2D/test/a549_hoechst_cellmask_test.zarr).\n",
    "It has pre-computed statistics for normalization, generated using the `viscy preprocess` CLI.\n",
    "\n",
    "Refer to our [preprint](https://doi.org/10.1101/2024.05.31.596901) for more details\n",
    "about how the dataset and model were generated.\n",
    "\n",
    "## User Data\n",
    "\n",
    "The VSCyto2D model only requires label-free images for inference.\n",
    "To run inference on your own data,\n",
    "convert them into the OME-Zarr data format using iohub or other\n",
    "[tools](https://ngff.openmicroscopy.org/tools/index.html#file-conversion),\n",
    "and run [pre-processing](https://github.com/mehta-lab/VisCy/blob/main/docs/usage.md#preprocessing)\n",
    "with the `viscy preprocess` CLI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f895d9",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "# Setup\n",
    "\n",
    "The commands below will install the required packages and download the example dataset and model checkpoint.\n",
    "It may take a few minutes to download all the files.\n",
    "\n",
    "## Setup Google Colab\n",
    "\n",
    "To run this quick-start guide using Google Colab,\n",
    "choose the 'T4' GPU runtime from the \"Connect\" dropdown menu\n",
    "in the upper-right corner of this notebook for faster execution.\n",
    "Using a GPU significantly speeds up running model inference, but CPU compute can also be used.\n",
    "\n",
    "## Setup Local Environment\n",
    "\n",
    "The commands below assume a Unix-like shell with `wget` installed.\n",
    "On Windows, the files can be downloaded manually from the URLs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66531eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install VisCy with the optional dependencies for this example\n",
    "# See the [repository](https://github.com/mehta-lab/VisCy) for more details\n",
    "!pip install \"viscy[metrics,visual]==0.3.0rc3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f20186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# restart kernel if running in Google Colab\n",
    "if \"get_ipython\" in globals():\n",
    "    session = get_ipython()\n",
    "    if \"google.colab\" in str(session):\n",
    "        print(\"Shutting down colab session.\")\n",
    "        session.kernel.do_shutdown(restart=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e551b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate installation\n",
    "!viscy --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3f9ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the example dataset\n",
    "!wget -m -np -nH --cut-dirs=5 -R \"index.html*\" \"https://public.czbiohub.org/comp.micro/viscy/VS_datasets/VSCyto2D/test/a549_hoechst_cellmask_test.zarr/\"\n",
    "# Download the model checkpoint\n",
    "!wget https://public.czbiohub.org/comp.micro/viscy/VS_models/VSCyto2D/VSCyto2D/epoch=399-step=23200.ckpt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862a5334",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "# Run Model Inference\n",
    "\n",
    "The following code will run inference on a single field of view (FOV) of the example dataset.\n",
    "This can also be achieved by using the VisCy CLI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c08b6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from iohub import open_ome_zarr\n",
    "from torchview import draw_graph\n",
    "\n",
    "from viscy.data.hcs import HCSDataModule\n",
    "from viscy.trainer import VisCyTrainer\n",
    "from viscy.transforms import NormalizeSampled\n",
    "from viscy.translation.engine import FcmaeUNet\n",
    "from viscy.translation.predict_writer import HCSPredictionWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126a92c9",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# NOTE: Nothing needs to be changed in this code block for the example to work.\n",
    "# If using your own data, please modify the paths below.\n",
    "\n",
    "# TODO: Set download paths, by default the working directory is used\n",
    "root_dir = Path()\n",
    "# TODO: modify the path to the input dataset\n",
    "input_data_path = root_dir / \"a549_hoechst_cellmask_test.zarr\"\n",
    "# TODO: modify the path to the model checkpoint\n",
    "model_ckpt_path = root_dir / \"epoch=399-step=23200.ckpt\"\n",
    "# TODO: modify the path to save the predictions\n",
    "output_path = root_dir / \"a549_prediction.zarr\"\n",
    "# TODO: Choose an FOV\n",
    "fov = \"0/0/0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf7344e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the data module for loading example images in prediction mode.\n",
    "# See API documentation for how to use it with a different dataset.\n",
    "# For example, View the documentation for the HCSDataModule class by running:\n",
    "?HCSDataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367241dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the data module to use the example dataset\n",
    "data_module = HCSDataModule(\n",
    "    # Path to HCS or Single-FOV OME-Zarr dataset\n",
    "    data_path=input_data_path / fov,\n",
    "    # Name of the input phase channel\n",
    "    source_channel=\"Phase3D\",\n",
    "    # Desired name of the output channels\n",
    "    target_channel=[\"Membrane\", \"Nuclei\"],\n",
    "    # Axial input size, 1 for 2D models\n",
    "    z_window_size=1,\n",
    "    # Batch size\n",
    "    # Adjust based on available memory (reduce if seeing OOM errors)\n",
    "    batch_size=8,\n",
    "    # Number of workers for data loading\n",
    "    # Set to 0 for Windows and macOS if running in a notebook,\n",
    "    # since multiprocessing only works with a `if __name__ == '__main__':` guard.\n",
    "    # On Linux, set it based on available CPU cores to maximize performance.\n",
    "    num_workers=4,\n",
    "    # Normalization strategy\n",
    "    # This one uses pre-computed statistics from `viscy preprocess`\n",
    "    # to subtract the median and divide by the interquartile range (IQR).\n",
    "    # It can also be replaced by other MONAI transforms.\n",
    "    normalizations=[\n",
    "        NormalizeSampled(\n",
    "            [\"Phase3D\"],\n",
    "            level=\"fov_statistics\",\n",
    "            subtrahend=\"median\",\n",
    "            divisor=\"iqr\",\n",
    "        )\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c7bb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the VSCyto2D model from the downloaded checkpoint\n",
    "# VSCyto2D is fine-tuned from a FCMAE-pretrained UNeXt2 model.\n",
    "# See this module for options to configure the model:\n",
    "from viscy.unet.networks.fcmae import FullyConvolutionalMAE\n",
    "\n",
    "?FullyConvolutionalMAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78921228",
   "metadata": {},
   "outputs": [],
   "source": [
    "vs_cyto_2d = FcmaeUNet.load_from_checkpoint(\n",
    "    # checkpoint path\n",
    "    model_ckpt_path,\n",
    "    model_config={\n",
    "        # number of input channels\n",
    "        # must match the number of channels in the input data\n",
    "        \"in_channels\": 1,\n",
    "        # number of output channels\n",
    "        # must match the number of target channels in the data module\n",
    "        \"out_channels\": 2,\n",
    "        # number of ConvNeXt v2 blocks in each stage of the encoder\n",
    "        \"encoder_blocks\": [3, 3, 9, 3],\n",
    "        # feature map channels in each stage of the encoder\n",
    "        \"dims\": [96, 192, 384, 768],\n",
    "        # number of ConvNeXt v2 blocks in each stage of the decoder\n",
    "        \"decoder_conv_blocks\": 2,\n",
    "        # kernel size in the stem layer\n",
    "        \"stem_kernel_size\": [1, 2, 2],\n",
    "        # axial size of the input image\n",
    "        # must match the Z-window size in the data module\n",
    "        \"in_stack_depth\": 1,\n",
    "        # whether to perform masking (for FCMAE pre-training)\n",
    "        \"pretraining\": False,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf92bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the model graph\n",
    "model_graph = draw_graph(\n",
    "    vs_cyto_2d,\n",
    "    (vs_cyto_2d.example_input_array),\n",
    "    graph_name=\"VSCyto2D\",\n",
    "    roll=True,\n",
    "    depth=3,\n",
    "    expand_nested=True,\n",
    ")\n",
    "\n",
    "model_graph.visual_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34af7266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the trainer for prediction\n",
    "# The trainer can be further configured to better utilize the available hardware,\n",
    "# For example using GPUs and half precision.\n",
    "# Callbacks can also be used to customize logging and prediction writing.\n",
    "# See the API documentation for more details:\n",
    "?VisCyTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99b705d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the trainer\n",
    "# The prediction writer callback will save the predictions to an OME-Zarr store\n",
    "trainer = VisCyTrainer(callbacks=[HCSPredictionWriter(output_path)])\n",
    "\n",
    "# Run prediction\n",
    "trainer.predict(model=vs_cyto_2d, datamodule=data_module, return_predictions=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20081d17",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "# Model Outputs\n",
    "\n",
    "The model outputs are also stored in an OME-Zarr store.\n",
    "It can be visualized in an image viewer such as [napari](https://napari.org/).\n",
    "Below we show a snapshot in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16c8d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read images from Zarr stores\n",
    "# Choose the ROI for better visualization\n",
    "y_slice = slice(0, 512)\n",
    "x_slice = slice(0, 512)\n",
    "\n",
    "# Open the prediction store and get the 2D images from 5D arrays (t,c,z,y,x)\n",
    "with open_ome_zarr(output_path / fov) as vs_store:\n",
    "    vs_nucleus = vs_store[0][0, 0, 0, y_slice, x_slice]\n",
    "    vs_membrane = vs_store[0][0, 1, 0, y_slice, x_slice]\n",
    "\n",
    "# Open the experimental fluorescence dataset\n",
    "with open_ome_zarr(input_data_path / fov) as fluor_store:\n",
    "    fluor_nucleus = fluor_store[0][0, 1, 0, y_slice, x_slice]\n",
    "    fluor_membrane = fluor_store[0][0, 2, 0, y_slice, x_slice]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb0d335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from cmap import Colormap\n",
    "from skimage.exposure import rescale_intensity\n",
    "\n",
    "\n",
    "def render_rgb(image: np.ndarray, colormap: Colormap):\n",
    "    image = rescale_intensity(image, out_range=(0, 1))\n",
    "    image = colormap(image)\n",
    "    return image\n",
    "\n",
    "\n",
    "# Render the images as RGB in false colors\n",
    "vs_nucleus_rgb = render_rgb(vs_nucleus, Colormap(\"bop_blue\"))\n",
    "vs_membrane_rgb = render_rgb(vs_membrane, Colormap(\"bop_orange\"))\n",
    "merged_vs = (vs_nucleus_rgb + vs_membrane_rgb).clip(0, 1)\n",
    "\n",
    "fluor_nucleus_rgb = render_rgb(fluor_nucleus, Colormap(\"green\"))\n",
    "fluor_membrane_rgb = render_rgb(fluor_membrane, Colormap(\"magenta\"))\n",
    "merged_fluor = (fluor_nucleus_rgb + fluor_membrane_rgb).clip(0, 1)\n",
    "\n",
    "# Plot\n",
    "# Show the individual channels and then fused in a grid\n",
    "fig, ax = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "# Virtual staining plots\n",
    "ax[0, 0].imshow(vs_nucleus_rgb)\n",
    "ax[0, 0].set_title(\"VS Nuclei\")\n",
    "ax[0, 1].imshow(vs_membrane_rgb)\n",
    "ax[0, 1].set_title(\"VS Membrane\")\n",
    "ax[0, 2].imshow(merged_vs)\n",
    "ax[0, 2].set_title(\"VS Nuclei+Membrane\")\n",
    "\n",
    "# Experimental fluorescence plots\n",
    "ax[1, 0].imshow(fluor_nucleus_rgb)\n",
    "ax[1, 0].set_title(\"Experimental Fluorescence Nuclei\")\n",
    "ax[1, 1].imshow(fluor_membrane_rgb)\n",
    "ax[1, 1].set_title(\"Experimental Fluorescence Membrane\")\n",
    "ax[1, 2].imshow(merged_fluor)\n",
    "ax[1, 2].set_title(\"Experimental Fluorescence Nuclei+Membrane\")\n",
    "\n",
    "# turnoff axis\n",
    "for a in ax.flatten():\n",
    "    a.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f8efb5",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Responsible Use\n",
    "\n",
    "We are committed to advancing the responsible development and use of artificial intelligence.\n",
    "Please follow our [Acceptable Use Policy](https://virtualcellmodels.cziscience.com/acceptable-use-policy) when engaging with our services.\n",
    "\n",
    "Should you have any security or privacy issues or questions related to the services,\n",
    "please reach out to our team at [security@chanzuckerberg.com](mailto:security@chanzuckerberg.com) or [privacy@chanzuckerberg.com](mailto:privacy@chanzuckerberg.com) respectively."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "all",
   "main_language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
