---
phase: 06-package-scaffold-shared-components
plan: 03
type: execute
wave: 2
depends_on: ["06-01"]
files_modified:
  - packages/viscy-models/src/viscy_models/unet/_layers/conv_block_2d.py
  - packages/viscy-models/src/viscy_models/unet/_layers/conv_block_3d.py
  - packages/viscy-models/src/viscy_models/unet/_layers/__init__.py
  - packages/viscy-models/tests/test_unet/test_layers.py
autonomous: true

must_haves:
  truths:
    - "from viscy_models.unet._layers import ConvBlock2D, ConvBlock3D works"
    - "ConvBlock2D forward pass produces correct output shape"
    - "ConvBlock3D forward pass produces correct output shape"
    - "ConvBlock2D/3D use register_modules/add_module pattern (NOT nn.ModuleList)"
    - "State dict keys match original ConvBlock2D/3D exactly (e.g., Conv2d_0, batch_norm_0)"
    - "All layer tests pass"
  artifacts:
    - path: "packages/viscy-models/src/viscy_models/unet/_layers/conv_block_2d.py"
      provides: "ConvBlock2D nn.Module"
      exports: ["ConvBlock2D"]
      min_lines: 100
    - path: "packages/viscy-models/src/viscy_models/unet/_layers/conv_block_3d.py"
      provides: "ConvBlock3D nn.Module"
      exports: ["ConvBlock3D"]
      min_lines: 100
    - path: "packages/viscy-models/src/viscy_models/unet/_layers/__init__.py"
      provides: "Public re-exports of ConvBlock2D and ConvBlock3D"
    - path: "packages/viscy-models/tests/test_unet/test_layers.py"
      provides: "Tests for ConvBlock2D and ConvBlock3D"
  key_links:
    - from: "packages/viscy-models/src/viscy_models/unet/_layers/conv_block_2d.py"
      to: "numpy"
      via: "np.linspace for filter step calculations"
      pattern: "np\\.linspace"
    - from: "packages/viscy-models/src/viscy_models/unet/_layers/conv_block_3d.py"
      to: "numpy"
      via: "np.linspace for filter step calculations"
      pattern: "np\\.linspace"
---

<objective>
Migrate ConvBlock2D and ConvBlock3D layers from v0.3.3 source into `viscy_models.unet._layers/` with snake_case filenames and full test coverage.

Purpose: These are the fundamental convolutional building blocks for the legacy Unet2d and Unet25d models (Phase 9). Migrating them now provides a stable import location.
Output: Two layer modules with forward-pass and state-dict-key tests.
</objective>

<execution_context>
@/Users/eduardo.hirata/.claude/get-shit-done/workflows/execute-plan.md
@/Users/eduardo.hirata/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/06-package-scaffold-shared-components/06-RESEARCH.md
@.planning/phases/06-package-scaffold-shared-components/06-01-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Migrate ConvBlock2D and ConvBlock3D to unet/_layers/</name>
  <files>
    packages/viscy-models/src/viscy_models/unet/_layers/conv_block_2d.py
    packages/viscy-models/src/viscy_models/unet/_layers/conv_block_3d.py
    packages/viscy-models/src/viscy_models/unet/_layers/__init__.py
  </files>
  <action>
Migrate ConvBlock2D and ConvBlock3D from v0.3.3 source. Source is available via `git show v0.3.3:viscy/unet/networks/layers/ConvBlock2D.py` and `git show v0.3.3:viscy/unet/networks/layers/ConvBlock3D.py`.

**conv_block_2d.py** -- Copy ConvBlock2D class from v0.3.3 `ConvBlock2D.py`:
- File renamed from PascalCase to snake_case (ConvBlock2D.py -> conv_block_2d.py)
- Class name stays `ConvBlock2D` (unchanged)
- Copy the ENTIRE class verbatim including:
  - `register_modules()` method using `add_module()` -- DO NOT change to nn.ModuleList
  - `model()` method
  - `forward()` method with residual logic
  - All `__init__` parameter defaults preserved exactly
- Imports: `import numpy as np`, `import torch`, `import torch.nn as nn`, `import torch.nn.functional as F`
- Add `__all__ = ["ConvBlock2D"]`
- Add module-level docstring: `"""2D convolutional block with configurable layer ordering for UNet architectures."""`

**conv_block_3d.py** -- Copy ConvBlock3D class from v0.3.3 `ConvBlock3D.py`:
- Same approach: file renamed to snake_case, class name unchanged
- Copy ENTIRE class verbatim including register_modules/add_module pattern
- Imports: same as ConvBlock2D
- Add `__all__ = ["ConvBlock3D"]`
- Add module-level docstring: `"""3D convolutional block with configurable layer ordering for UNet architectures."""`

**_layers/__init__.py** -- Update to re-export:
```python
"""Shared layer implementations for UNet models."""
from viscy_models.unet._layers.conv_block_2d import ConvBlock2D
from viscy_models.unet._layers.conv_block_3d import ConvBlock3D

__all__ = ["ConvBlock2D", "ConvBlock3D"]
```

CRITICAL:
- The `register_modules` method with `add_module(f"{name}_{str(i)}", module)` creates state dict keys like `Conv2d_0`, `Conv2d_1`, `batch_norm_0`, etc. This pattern MUST be preserved exactly. DO NOT refactor to nn.ModuleList.
- The `ConvBlock3D.__init__` has a `padding=None` parameter with custom padding logic. Preserve this exactly.
- ConvBlock3D registers dropout modules via `self.register_modules(self.drop_list, "dropout")` while ConvBlock2D does NOT register dropout (it's just stored in a list). Preserve this asymmetry.

After writing, run:
- `uv run ruff format packages/viscy-models/src/viscy_models/unet/_layers/`
- `uv run ruff check packages/viscy-models/src/viscy_models/unet/_layers/ --fix`

Note: ruff may flag some style issues in the legacy code. Fix only formatting issues (line length, trailing whitespace). Do NOT change logic, variable names, or module registration patterns. If ruff flags mutable-default-argument (B006), the ConvBlocks use `dropout=False` and `kernel_size=3` / `kernel_size=(3,3,3)` which are immutable -- these are fine. If ruff flags the numpy import (NPY), keep it as-is since ConvBlocks genuinely use np.linspace.
  </action>
  <verify>
`uv run --package viscy-models python -c "from viscy_models.unet._layers import ConvBlock2D, ConvBlock3D; print('layers OK')"` succeeds.
`uv run --package viscy-models python -c "
from viscy_models.unet._layers import ConvBlock2D
m = ConvBlock2D(16, 32); sd = m.state_dict()
assert any('Conv2d_0' in k for k in sd.keys()), f'Missing Conv2d_0 in {list(sd.keys())}'
print('state dict keys OK')
"` succeeds (confirms register_modules pattern preserved).
  </verify>
  <done>ConvBlock2D and ConvBlock3D are importable from viscy_models.unet._layers. State dict keys use the original add_module naming pattern. Code is ruff-clean.</done>
</task>

<task type="auto">
  <name>Task 2: Write tests for ConvBlock2D and ConvBlock3D</name>
  <files>
    packages/viscy-models/tests/test_unet/test_layers.py
  </files>
  <action>
Write comprehensive tests for ConvBlock2D and ConvBlock3D covering forward pass, state dict keys, and configuration options.

**test_layers.py**:

ConvBlock2D tests:
- `test_conv_block_2d_default_forward`: Create `ConvBlock2D(16, 32)`. Input: `torch.randn(1, 16, 64, 64)`. Assert output shape is `(1, 32, 64, 64)` (same spatial dims due to "same" padding, channels change from 16 to 32).
- `test_conv_block_2d_state_dict_keys`: Create `ConvBlock2D(16, 32)`. Get state dict keys. Assert that `Conv2d_0.weight` and `batch_norm_0.weight` are present (confirming register_modules pattern).
- `test_conv_block_2d_residual`: Create `ConvBlock2D(16, 32, residual=True)`. Forward pass should work. Create `ConvBlock2D(16, 32, residual=False)`. Forward pass should also work. Both produce shape `(1, 32, 64, 64)`.
- `test_conv_block_2d_filter_steps_linear`: Create `ConvBlock2D(16, 64, filter_steps="linear")`. Input: `torch.randn(1, 16, 32, 32)`. Assert output shape is `(1, 64, 32, 32)`.
- `test_conv_block_2d_instance_norm`: Create `ConvBlock2D(16, 32, norm="instance")`. Assert state dict contains keys with `instance_norm`.

ConvBlock3D tests:
- `test_conv_block_3d_default_forward`: Create `ConvBlock3D(8, 16)`. Input: `torch.randn(1, 8, 5, 32, 32)`. With default padding="same" equivalent, output spatial dims should be preserved. Assert output shape is `(1, 16, 5, 32, 32)`.
- `test_conv_block_3d_state_dict_keys`: Create `ConvBlock3D(8, 16)`. Assert `Conv3d_0.weight` and `batch_norm_0.weight` are in state dict keys.
- `test_conv_block_3d_dropout_registered`: Create `ConvBlock3D(8, 16, dropout=0.5)`. Assert `dropout_0` is in state dict keys or named_modules (ConvBlock3D registers dropout unlike 2D). Check via `dict(model.named_modules())` that 'dropout_0' exists.
- `test_conv_block_3d_layer_order`: Create `ConvBlock3D(8, 16, layer_order="cna")`. Forward pass succeeds with `torch.randn(1, 8, 5, 16, 16)`.

Run: `uv run --package viscy-models pytest packages/viscy-models/tests/test_unet/test_layers.py -v`
  </action>
  <verify>
`uv run --package viscy-models pytest packages/viscy-models/tests/test_unet/test_layers.py -v` passes all tests.
At least 8 tests total covering both ConvBlock2D and ConvBlock3D.
  </verify>
  <done>ConvBlock2D and ConvBlock3D have forward-pass tests, state-dict-key tests, and configuration variant tests. All pass.</done>
</task>

</tasks>

<verification>
1. `from viscy_models.unet._layers import ConvBlock2D, ConvBlock3D` works
2. State dict keys use add_module naming (Conv2d_0, batch_norm_0, etc.)
3. register_modules pattern preserved (NOT nn.ModuleList)
4. All layer tests pass
5. ruff format/check clean
</verification>

<success_criteria>
- UNET-05: ConvBlock2D/3D migrated to unet/_layers/ (renamed from PascalCase files)
- State dict keys identical to original implementation
- register_modules/add_module pattern preserved verbatim
- Full test coverage for both blocks
</success_criteria>

<output>
After completion, create `.planning/phases/06-package-scaffold-shared-components/06-03-SUMMARY.md`
</output>
