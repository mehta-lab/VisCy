---
phase: 24-dataset-datamodule
plan: 02
type: tdd
wave: 2
depends_on: ["24-01"]
files_modified:
  - applications/dynaclr/src/dynaclr/datamodule.py
  - applications/dynaclr/tests/test_datamodule.py
  - applications/dynaclr/src/dynaclr/__init__.py
autonomous: true

must_haves:
  truths:
    - "MultiExperimentDataModule wires FlexibleBatchSampler + MultiExperimentTripletDataset + ChannelDropout + ThreadDataLoader with collate_fn=lambda x: x"
    - "Train/val split is by whole experiments, not individual FOVs"
    - "All sampling, loss, and augmentation hyperparameters (tau_range, tau_decay, experiment_aware, condition_balanced, temporal_enrichment, hcl_beta, channel_dropout_prob) are exposed as __init__ parameters"
    - "MultiExperimentDataModule and MultiExperimentTripletDataset are importable from dynaclr top-level"
  artifacts:
    - path: "applications/dynaclr/src/dynaclr/datamodule.py"
      provides: "MultiExperimentDataModule LightningDataModule"
      exports: ["MultiExperimentDataModule"]
    - path: "applications/dynaclr/tests/test_datamodule.py"
      provides: "TDD tests for DataModule"
      contains: "test_train_val_split_by_experiment"
    - path: "applications/dynaclr/src/dynaclr/__init__.py"
      provides: "Updated top-level exports"
      contains: "MultiExperimentDataModule"
  key_links:
    - from: "applications/dynaclr/src/dynaclr/datamodule.py"
      to: "applications/dynaclr/src/dynaclr/dataset.py"
      via: "creates MultiExperimentTripletDataset for train and val"
      pattern: "MultiExperimentTripletDataset"
    - from: "applications/dynaclr/src/dynaclr/datamodule.py"
      to: "packages/viscy-data/src/viscy_data/sampler.py"
      via: "creates FlexibleBatchSampler as batch_sampler for train DataLoader"
      pattern: "FlexibleBatchSampler"
    - from: "applications/dynaclr/src/dynaclr/datamodule.py"
      to: "packages/viscy-data/src/viscy_data/channel_dropout.py"
      via: "applies ChannelDropout in on_after_batch_transfer"
      pattern: "ChannelDropout"
---

<objective>
Implement MultiExperimentDataModule that wires together all composable sampling components (FlexibleBatchSampler, MultiExperimentTripletDataset, ChannelDropout, ThreadDataLoader) with train/val split by experiments and full Lightning CLI configurability.

Purpose: This DataModule is the final composition layer that exposes all sampling, augmentation, and loss hyperparameters as CLI-configurable parameters, enabling multi-experiment DynaCLR training with a single YAML config.

Output: `datamodule.py` with MultiExperimentDataModule class, TDD test suite, and updated `__init__.py` exports.
</objective>

<execution_context>
@/Users/eduardo.hirata/.claude/get-shit-done/workflows/execute-plan.md
@/Users/eduardo.hirata/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/24-dataset-datamodule/24-01-SUMMARY.md

# Key source files
@applications/dynaclr/src/dynaclr/engine.py         # ContrastiveModule expects TripletSample from on_after_batch_transfer
@applications/dynaclr/src/dynaclr/dataset.py         # MultiExperimentTripletDataset (from Plan 01)
@applications/dynaclr/src/dynaclr/index.py           # MultiExperimentIndex builds tracks + valid_anchors
@applications/dynaclr/src/dynaclr/experiment.py       # ExperimentConfig, ExperimentRegistry, from_yaml
@applications/dynaclr/src/dynaclr/tau_sampling.py     # sample_tau for variable temporal offset
@packages/viscy-data/src/viscy_data/sampler.py         # FlexibleBatchSampler with experiment-aware batching
@packages/viscy-data/src/viscy_data/channel_dropout.py # ChannelDropout nn.Module
@packages/viscy-data/src/viscy_data/triplet.py         # Existing TripletDataModule pattern (ThreadDataLoader, collate_fn, on_after_batch_transfer)
@packages/viscy-data/src/viscy_data/_utils.py          # _transform_channel_wise, _scatter_channels, _gather_channels
</context>

<tasks>

<task type="auto">
  <name>Task 1: TDD -- MultiExperimentDataModule with experiment-level split and component wiring</name>
  <files>
    applications/dynaclr/src/dynaclr/datamodule.py
    applications/dynaclr/tests/test_datamodule.py
  </files>
  <action>
**RED phase -- Write failing tests first in `test_datamodule.py`:**

Create test file with synthetic/mocked experiments. Since MultiExperimentDataModule does heavy I/O through MultiExperimentIndex -> zarr, use mocking or minimal synthetic zarr stores.

**Test cases** (minimum 5 tests):

1. `test_init_exposes_all_hyperparameters`: Instantiate MultiExperimentDataModule with all hyperparameters explicitly set (tau_range, tau_decay_rate, experiment_aware, condition_balanced, temporal_enrichment, temporal_window_hours, temporal_global_fraction, hcl_beta, channel_dropout_prob, channel_dropout_channels, batch_size, num_workers, leaky). Assert all values stored correctly on the instance. This validates DATA-05.

2. `test_train_val_split_by_experiment`: With a registry of 4 experiments and val_experiments=["exp_c", "exp_d"], verify that after setup("fit"):
   - `train_dataset.index` contains only experiments NOT in val_experiments
   - `val_dataset.index` contains only val_experiments
   - No FOV from a val experiment appears in train, and vice versa
   This validates DATA-04.

3. `test_train_dataloader_uses_flexible_batch_sampler`: After setup, verify `train_dataloader()` returns a ThreadDataLoader whose `batch_sampler` is a FlexibleBatchSampler with the configured experiment_aware, condition_balanced, temporal_enrichment settings. Verify `collate_fn` is the identity lambda. This validates DATA-03.

4. `test_val_dataloader_no_batch_sampler`: Verify val_dataloader uses simple sequential loading (no FlexibleBatchSampler), since validation should be deterministic.

5. `test_on_after_batch_transfer_applies_channel_dropout_and_transforms`: Create a mock batch dict with "anchor" and "positive" Tensors. Call `on_after_batch_transfer(batch, 0)`. Verify:
   - Output still has "anchor" and "positive" keys as Tensors
   - norm_meta keys are consumed (removed from output)
   - ChannelDropout is applied (in training mode, specified channels may be zeroed)
   - Transforms (normalizations + augmentations + final crop) are applied

6. `test_channel_dropout_integration`: Set channel_dropout_prob=1.0 for channel 1. After on_after_batch_transfer in training mode, verify channel 1 of both anchor and positive is all zeros. In eval mode, verify channel 1 is preserved.

**GREEN phase -- Implement `datamodule.py`:**

```python
class MultiExperimentDataModule(LightningDataModule):
    """Lightning DataModule for multi-experiment DynaCLR training.

    Composes MultiExperimentIndex, MultiExperimentTripletDataset,
    FlexibleBatchSampler, ChannelDropout, and ThreadDataLoader into
    a fully configurable training pipeline.

    Parameters
    ----------
    experiments_yaml : str
        Path to YAML config for ExperimentRegistry.from_yaml().
    z_range : tuple[int, int]
        Z-slice range (start, stop) for data loading.
    yx_patch_size : tuple[int, int]
        Initial YX patch size for cell patch extraction.
    final_yx_patch_size : tuple[int, int]
        Final YX patch size after cropping (output size).
    val_experiments : list[str]
        Experiment names to use for validation (rest are training).
    tau_range : tuple[float, float]
        (min_hours, max_hours) for temporal positive sampling.
    tau_decay_rate : float
        Exponential decay rate for tau sampling. Default: 2.0.
    batch_size : int
        Batch size. Default: 128.
    num_workers : int
        Thread workers for ThreadDataLoader. Default: 1.
    # --- Sampling hyperparameters (passed to FlexibleBatchSampler) ---
    experiment_aware : bool
        Restrict each batch to a single experiment. Default: True.
    condition_balanced : bool
        Balance conditions within each batch. Default: True.
    leaky : float
        Fraction of cross-experiment samples. Default: 0.0.
    temporal_enrichment : bool
        Concentrate around focal HPI. Default: False.
    temporal_window_hours : float
        Half-width of focal window. Default: 2.0.
    temporal_global_fraction : float
        Global fraction for temporal enrichment. Default: 0.3.
    experiment_weights : dict[str, float] | None
        Per-experiment sampling weights. Default: None (proportional).
    condition_ratio : dict[str, float] | None
        Per-condition target ratio. Default: None (equal).
    # --- Augmentation hyperparameters ---
    channel_dropout_channels : list[int]
        Channel indices to dropout. Default: [1] (fluorescence).
    channel_dropout_prob : float
        Dropout probability. Default: 0.5.
    normalizations : list[MapTransform]
        Normalization transforms. Default: [].
    augmentations : list[MapTransform]
        Augmentation transforms. Default: [].
    # --- Loss hyperparameters (informational, for CLI discoverability) ---
    hcl_beta : float
        Hard-negative concentration beta. Default: 0.5.
        NOTE: This is stored for YAML discoverability but the actual
        NTXentHCL instance is configured on ContrastiveModule, not here.
    # --- Other ---
    cache_pool_bytes : int
        Tensorstore cache pool size. Default: 0.
    seed : int
        RNG seed for FlexibleBatchSampler. Default: 0.
    include_wells : list[str] | None
        Only include these wells. Default: None.
    exclude_fovs : list[str] | None
        Exclude these FOVs. Default: None.
    """
```

Key implementation details:

1. **`__init__`**: Store all hyperparameters. Do NOT build index or dataset yet (that happens in `setup()`). Create ChannelDropout module: `self.channel_dropout = ChannelDropout(channels=channel_dropout_channels, p=channel_dropout_prob)`. Store normalizations and augmentations for transform pipeline.

2. **`setup(stage)`**: For "fit" stage:
   - Load registry: `ExperimentRegistry.from_yaml(self.experiments_yaml)`
   - Split experiments: `train_exps` = experiments NOT in val_experiments, `val_exps` = experiments in val_experiments
   - Create separate registries for train and val (or filter index by experiment)
   - Build `MultiExperimentIndex` for train experiments and val experiments separately, each with their own `valid_anchors`
   - Create `MultiExperimentTripletDataset` for train and val

3. **`train_dataloader()`**:
   - Create `FlexibleBatchSampler` from `self.train_dataset.index.valid_anchors` with all sampling hyperparameters
   - Return `ThreadDataLoader(self.train_dataset, batch_sampler=sampler, use_thread_workers=True, num_workers=self.num_workers, collate_fn=lambda x: x)`
   - Note: when using batch_sampler, do NOT pass batch_size or shuffle

4. **`val_dataloader()`**:
   - Simple ThreadDataLoader with batch_size, shuffle=False, collate_fn=lambda x: x
   - No FlexibleBatchSampler for validation (deterministic)

5. **`on_after_batch_transfer(batch, dataloader_idx)`**:
   - If batch is a Tensor (example_input_array), return as-is
   - For each key in ["anchor", "positive", "negative"]:
     - Apply `_transform_channel_wise` with normalizations + augmentations + final_crop (same pattern as existing TripletDataModule.on_after_batch_transfer)
     - Remove norm_meta keys after transforms
   - Apply `self.channel_dropout` to both "anchor" and "positive" (only during training via nn.Module train/eval mode)
   - Return transformed batch

6. **`_final_crop()`**: Create BatchedCenterSpatialCropd for final cropping from initial to final patch size.

7. **Transform pipeline**:
   - `_augmentation_transform = Compose(normalizations + augmentations + [final_crop])`
   - `_no_augmentation_transform = Compose(normalizations + [final_crop])`
   - Training uses augmentation transform for anchor (if tau > 0) and positive
   - Validation uses no-augmentation transform

**IMPORTANT design decisions to follow:**
- Train/val split is by EXPERIMENT (whole experiments), not by FOV. This is per STATE.md decision.
- `collate_fn=lambda x: x` because __getitems__ already returns a batched dict (not individual samples)
- FlexibleBatchSampler only for training. Validation is sequential.
- ChannelDropout applied AFTER transforms (consistent with Phase 23 design: after scatter/gather augmentation chain)
- `hcl_beta` is stored on DataModule for YAML discoverability but the actual loss is configured on ContrastiveModule. The DataModule doesn't create or own the loss.

**REFACTOR phase**: Clean up, ensure all __init__ params have docstrings, verify Lightning CLI compatibility (all params are simple types or have type hints that jsonargparse can handle).
  </action>
  <verify>
Run: `cd /Users/eduardo.hirata/Documents/repos/VisCy && uv run --package dynaclr pytest applications/dynaclr/tests/test_datamodule.py -v`

All tests pass. Verify at least 5 test cases exist and pass.
  </verify>
  <done>
MultiExperimentDataModule wires FlexibleBatchSampler + Dataset + ChannelDropout + ThreadDataLoader with correct collate_fn. Train/val split is by whole experiments. All hyperparameters are exposed as __init__ parameters. At least 5 TDD tests pass.
  </done>
</task>

<task type="auto">
  <name>Task 2: Update __init__.py exports for MultiExperimentTripletDataset and MultiExperimentDataModule</name>
  <files>
    applications/dynaclr/src/dynaclr/__init__.py
  </files>
  <action>
Update `applications/dynaclr/src/dynaclr/__init__.py` to export both new classes:

Add these imports:
```python
from dynaclr.dataset import MultiExperimentTripletDataset
from dynaclr.datamodule import MultiExperimentDataModule
```

Add to `__all__`:
```python
"MultiExperimentTripletDataset",
"MultiExperimentDataModule",
```

Verify import works:
```bash
uv run --package dynaclr python -c "from dynaclr import MultiExperimentTripletDataset, MultiExperimentDataModule; print('OK')"
```
  </action>
  <verify>
Run: `uv run --package dynaclr python -c "from dynaclr import MultiExperimentTripletDataset, MultiExperimentDataModule; print('exports OK')"`

Run: `uv run --package dynaclr python -c "import dynaclr; assert 'MultiExperimentTripletDataset' in dynaclr.__all__; assert 'MultiExperimentDataModule' in dynaclr.__all__; print('__all__ OK')"`
  </verify>
  <done>
Both MultiExperimentTripletDataset and MultiExperimentDataModule are importable from `dynaclr` top-level and listed in `__all__`.
  </done>
</task>

</tasks>

<verification>
1. `uv run --package dynaclr pytest applications/dynaclr/tests/test_datamodule.py -v` -- all tests pass
2. `uv run --package dynaclr python -c "from dynaclr import MultiExperimentDataModule, MultiExperimentTripletDataset; print('OK')"` -- imports work
3. Verify train/val split is by experiment (test_train_val_split_by_experiment passes)
4. Verify FlexibleBatchSampler is used for training (test_train_dataloader_uses_flexible_batch_sampler passes)
5. Verify ChannelDropout integration (test_channel_dropout_integration passes)
6. Verify all hyperparameters are exposed (test_init_exposes_all_hyperparameters passes)
</verification>

<success_criteria>
- MultiExperimentDataModule composes all sampling components correctly
- Train/val split by whole experiments is verified
- All hyperparameters exposed for Lightning CLI YAML configuration
- ChannelDropout + transforms applied in on_after_batch_transfer
- Both new classes importable from dynaclr top-level
- All TDD tests pass
</success_criteria>

<output>
After completion, create `.planning/phases/24-dataset-datamodule/24-02-SUMMARY.md`
</output>
