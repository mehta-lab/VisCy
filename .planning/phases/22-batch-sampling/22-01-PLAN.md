---
phase: 22-batch-sampling
plan: 01
type: tdd
wave: 1
depends_on: []
files_modified:
  - packages/viscy-data/src/viscy_data/sampler.py
  - packages/viscy-data/tests/test_sampler.py
autonomous: true

must_haves:
  truths:
    - "With experiment_aware=True, every batch contains cells from only a single experiment"
    - "With condition_balanced=True, each batch has approximately equal representation of each condition"
    - "With leaky > 0.0, a configurable fraction of cross-experiment samples appear in experiment-restricted batches"
    - "With experiment_aware=False, batches draw from all experiments freely"
    - "Small groups fall back to replacement sampling with a logged warning rather than crashing"
  artifacts:
    - path: "packages/viscy-data/src/viscy_data/sampler.py"
      provides: "FlexibleBatchSampler class with experiment-aware, condition-balanced, and leaky mixing"
      exports: ["FlexibleBatchSampler"]
      min_lines: 150
    - path: "packages/viscy-data/tests/test_sampler.py"
      provides: "TDD test suite for core sampling axes"
      min_lines: 200
  key_links:
    - from: "packages/viscy-data/tests/test_sampler.py"
      to: "packages/viscy-data/src/viscy_data/sampler.py"
      via: "from viscy_data.sampler import FlexibleBatchSampler"
      pattern: "from viscy_data\\.sampler import FlexibleBatchSampler"
    - from: "packages/viscy-data/src/viscy_data/sampler.py"
      to: "torch.utils.data.Sampler"
      via: "Sampler[list[int]] subclass"
      pattern: "class FlexibleBatchSampler\\(Sampler"
---

<objective>
TDD implementation of FlexibleBatchSampler core: experiment-aware batching (SAMP-01), condition balancing (SAMP-02), and leaky experiment mixing (SAMP-05).

Purpose: Establish the sampler class with cascade batch construction (experiment -> condition -> sample) and the Sampler[list[int]] protocol. These three axes form the foundation that Plan 02 extends with temporal enrichment and DDP.

Output: Working FlexibleBatchSampler that yields experiment-restricted, condition-balanced batches with optional leaky mixing, plus comprehensive TDD test suite.
</objective>

<execution_context>
@/Users/eduardo.hirata/.claude/get-shit-done/workflows/execute-plan.md
@/Users/eduardo.hirata/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/22-batch-sampling/22-RESEARCH.md
@packages/viscy-data/src/viscy_data/distributed.py
@packages/viscy-data/src/viscy_data/__init__.py
@applications/dynaclr/src/dynaclr/index.py
</context>

<feature>
  <name>FlexibleBatchSampler core: experiment-aware + condition-balanced + leaky mixing</name>
  <files>packages/viscy-data/src/viscy_data/sampler.py, packages/viscy-data/tests/test_sampler.py</files>
  <behavior>
    FlexibleBatchSampler(valid_anchors, batch_size, experiment_aware, condition_balanced, leaky, ...) implements Sampler[list[int]].

    Cases:
    - experiment_aware=True, 2 experiments, batch_size=8 -> every batch indices map to exactly 1 experiment
    - experiment_aware=True, 3 experiments, many batches -> all experiments appear at least once (proportional selection)
    - experiment_aware=False -> batches may contain indices from multiple experiments
    - condition_balanced=True, 2 conditions -> each batch has ~50% of each condition (within +/-20% tolerance for small batches)
    - condition_balanced=True, 3 conditions -> each batch has ~33% of each condition
    - condition_balanced=False -> no condition constraint, random sampling from pool
    - leaky=0.0, experiment_aware=True -> 0 cross-experiment indices in each batch
    - leaky=0.2, experiment_aware=True, batch_size=10 -> ~2 indices from other experiments per batch
    - leaky=0.0, experiment_aware=False -> leaky has no effect
    - batch_size > smallest group -> falls back to replacement sampling, does not crash
    - __len__ returns total_batches // num_replicas (single-process: num_replicas=1)
    - __iter__ yields list[int] (not individual ints)
    - Deterministic: same seed + same epoch -> same batch sequence
    - set_epoch(n) changes the RNG seed for next __iter__ call
  </behavior>
  <implementation>
    Create sampler.py with FlexibleBatchSampler(Sampler[list[int]]):

    __init__ params:
    - valid_anchors: pd.DataFrame (must have "experiment" and "condition" columns)
    - batch_size: int = 128
    - experiment_aware: bool = True
    - leaky: float = 0.0 (fraction, 0.0-1.0)
    - experiment_weights: dict[str, float] | None = None (default: proportional to group size)
    - condition_balanced: bool = True
    - condition_ratio: dict[str, float] | None = None (default: equal across conditions)
    - num_replicas: int = 1
    - rank: int = 0
    - seed: int = 0
    - drop_last: bool = True

    At __init__, call _precompute_groups() to build:
    - self._experiment_indices: dict[str, np.ndarray] from valid_anchors.groupby("experiment")
    - self._exp_cond_indices: dict[tuple[str, str], np.ndarray] from valid_anchors.groupby(["experiment", "condition"])
    - self._all_indices: np.arange(len(valid_anchors))
    - self._experiment_names: list[str]
    - Emit logging.warning if any experiment group < batch_size

    _build_one_batch(rng: np.random.Generator) -> list[int]:
    1. If experiment_aware: pick experiment via rng.choice(names, p=weights)
       - Default weights: proportional to len(experiment_indices[name]) / total
       - Custom weights: normalize experiment_weights dict
       Then pool = self._experiment_indices[chosen_exp]
    2. If not experiment_aware: pool = self._all_indices, chosen_exp = None
    3. If leaky > 0.0 and experiment_aware:
       n_leak = int(batch_size * leaky)
       n_primary = batch_size - n_leak
       other_indices = np.concatenate([v for k, v in self._experiment_indices.items() if k != chosen_exp])
       leak_sample = rng.choice(other_indices, size=min(n_leak, len(other_indices)), replace=len(other_indices) < n_leak)
    4. If condition_balanced and chosen_exp is not None:
       conditions_in_exp = [c for (e, c) in self._exp_cond_indices if e == chosen_exp]
       ratios = condition_ratio or {c: 1.0/len(conditions_in_exp) for c in conditions_in_exp}
       For each condition: sample int(n_primary * ratio) indices from self._exp_cond_indices[(chosen_exp, cond)]
       Use replace=True if pool < needed (with warning at init time, not per-batch)
       Concatenate all condition samples
    5. If condition_balanced and chosen_exp is None (experiment_aware=False):
       Same logic but across all conditions globally
    6. If not condition_balanced: rng.choice(pool, size=n_primary, replace=len(pool) < n_primary)
    7. Concatenate primary + leak samples, return as list[int]

    __iter__: rng = np.random.default_rng(seed + epoch), generate total_batches, slice by rank
    __len__: math.ceil((len(valid_anchors) // batch_size) / num_replicas)
    set_epoch(epoch): self.epoch = epoch

    Use numpy RNG throughout (np.random.default_rng), NOT global numpy state or torch Generator.
    Do NOT import from dynaclr -- this is in the reusable viscy-data package.
    Use logging.getLogger(__name__) for warnings about small groups.
  </implementation>
</feature>

<verification>
cd /Users/eduardo.hirata/Documents/repos/VisCy && uv run pytest packages/viscy-data/tests/test_sampler.py -v
All tests pass. No ruff lint errors: uv run ruff check packages/viscy-data/src/viscy_data/sampler.py
</verification>

<success_criteria>
- FlexibleBatchSampler importable from viscy_data.sampler
- experiment_aware=True restricts every batch to one experiment (verified over 50+ batches in tests)
- condition_balanced=True produces ~equal condition representation per batch (statistical tolerance)
- leaky=0.2 injects ~20% cross-experiment samples
- Deterministic: same seed+epoch reproduces identical batch sequence
- All tests pass, no lint errors
</success_criteria>

<output>
After completion, create `.planning/phases/22-batch-sampling/22-01-SUMMARY.md`
</output>
