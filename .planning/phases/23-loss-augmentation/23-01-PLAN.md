---
phase: 23-loss-augmentation
plan: 01
type: tdd
wave: 1
depends_on: []
files_modified:
  - applications/dynaclr/src/dynaclr/loss.py
  - applications/dynaclr/tests/test_loss.py
autonomous: true

must_haves:
  truths:
    - "NTXentHCL with beta=0.0 produces numerically identical results to NTXentLoss for the same embeddings and labels"
    - "NTXentHCL with beta>0 concentrates the loss on hard negatives, producing a different (higher) loss value than beta=0"
    - "NTXentHCL returns a scalar tensor with gradients that backpropagates without error"
    - "NTXentHCL passes isinstance(loss, NTXentLoss) so the existing ContrastiveModule training_step NTXent code path activates without modification"
    - "NTXentHCL is configurable via Lightning CLI YAML with class_path: dynaclr.loss.NTXentHCL"
  artifacts:
    - path: "applications/dynaclr/src/dynaclr/loss.py"
      provides: "NTXentHCL nn.Module with hard-negative concentration"
      exports: ["NTXentHCL"]
      min_lines: 60
    - path: "applications/dynaclr/tests/test_loss.py"
      provides: "TDD test suite for NTXentHCL"
      min_lines: 120
  key_links:
    - from: "applications/dynaclr/tests/test_loss.py"
      to: "applications/dynaclr/src/dynaclr/loss.py"
      via: "from dynaclr.loss import NTXentHCL"
      pattern: "from dynaclr\\.loss import NTXentHCL"
    - from: "applications/dynaclr/src/dynaclr/loss.py"
      to: "pytorch_metric_learning.losses"
      via: "NTXentHCL subclasses NTXentLoss"
      pattern: "class NTXentHCL\\(NTXentLoss\\)"
    - from: "applications/dynaclr/src/dynaclr/engine.py"
      to: "applications/dynaclr/src/dynaclr/loss.py"
      via: "isinstance(self.loss_function, NTXentLoss) check passes for NTXentHCL"
      pattern: "isinstance.*NTXentLoss"
---

<objective>
TDD implementation of NTXentHCL: NT-Xent loss with hard-negative concentration (LOSS-01, LOSS-02, LOSS-03).

Purpose: Provide a contrastive loss that up-weights hard negatives via a beta parameter, improving representation learning for cellular dynamics where many negatives are trivially easy. When beta=0.0 it falls back to standard NT-Xent, ensuring backward compatibility.

Output: `loss.py` with NTXentHCL class and `test_loss.py` with comprehensive TDD coverage.
</objective>

<execution_context>
@/Users/eduardo.hirata/.claude/get-shit-done/workflows/execute-plan.md
@/Users/eduardo.hirata/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@applications/dynaclr/src/dynaclr/engine.py
@applications/dynaclr/src/dynaclr/__init__.py
@applications/dynaclr/tests/test_training_integration.py
</context>

<feature>
  <name>NTXentHCL: NT-Xent with Hard-Negative Concentration</name>
  <files>
    applications/dynaclr/src/dynaclr/loss.py
    applications/dynaclr/tests/test_loss.py
  </files>
  <behavior>
    NTXentHCL(temperature=0.07, beta=0.5) is an nn.Module that subclasses NTXentLoss from pytorch_metric_learning.

    The HCL formula modifies the NT-Xent denominator. For anchor i with positive p(i):
      Standard NT-Xent: L_i = -log( exp(sim(i, p(i))/tau) / sum_k!=i exp(sim(i, k)/tau) )
      HCL modifies the negative term: each negative similarity is reweighted by exp(beta * sim(i, k))
      Denominator becomes: sum_k!=i [ exp(beta * sim(i, k)) * exp(sim(i, k)/tau) ]
      Which simplifies to: sum_k!=i [ exp(sim(i, k) * (beta + 1/tau)) ]
      When beta=0.0: denominator = sum_k!=i exp(sim(i, k)/tau) = standard NT-Xent

    Calling convention: loss = ntxent_hcl(embeddings, labels)
      - embeddings: Tensor of shape (2N, D) where first N are anchors, next N are positives
      - labels: Tensor of shape (2N,) where labels[i] == labels[i+N] for positive pairs
      - Returns: scalar Tensor with grad_fn

    Cases:
      - beta=0.0, same inputs as NTXentLoss -> numerically identical output (atol=1e-6)
      - beta=0.5, typical embeddings -> loss value differs from beta=0 (hard negatives upweighted)
      - beta=1.0, embeddings with one hard negative -> loss is higher than beta=0
      - Gradient flows: loss.backward() completes without error, encoder params have .grad
      - isinstance(NTXentHCL(...), NTXentLoss) returns True
      - isinstance(NTXentHCL(...), nn.Module) returns True
      - temperature parameter controls scale (lower temp -> sharper distribution)
      - Works on both CPU and CUDA (if available)
      - Batch size 1 edge case: does not crash (though loss may be degenerate)
      - Large batch (128 pairs): completes in reasonable time, no numerical overflow
  </behavior>
  <implementation>
    RED phase:
      Create test_loss.py with these test cases:

      1. test_ntxent_hcl_is_ntxent_subclass -- isinstance(NTXentHCL(), NTXentLoss) is True
      2. test_ntxent_hcl_is_nn_module -- isinstance(NTXentHCL(), nn.Module) is True
      3. test_ntxent_hcl_beta_zero_matches_standard -- Create NTXentLoss(temperature=0.1) and NTXentHCL(temperature=0.1, beta=0.0). Feed identical random embeddings (32, 128) with matching labels. Assert torch.allclose(loss_hcl, loss_standard, atol=1e-6).
      4. test_ntxent_hcl_beta_positive_differs -- NTXentHCL(beta=0.5) produces different loss than NTXentHCL(beta=0.0) on same inputs
      5. test_ntxent_hcl_returns_scalar_with_grad -- loss.shape == (), loss.requires_grad is True
      6. test_ntxent_hcl_backward_passes -- loss.backward() runs, check a parameter has .grad
      7. test_ntxent_hcl_hard_negatives_increase_loss -- Construct embeddings where one negative is very similar to anchor. beta>0 should give higher loss than beta=0.
      8. test_ntxent_hcl_temperature_effect -- Lower temperature with beta>0 produces different loss than higher temperature
      9. test_ntxent_hcl_batch_size_one -- Single pair, does not crash
      10. test_ntxent_hcl_large_batch -- 128 pairs, completes without NaN or Inf
      11. test_ntxent_hcl_default_parameters -- NTXentHCL() has temperature=0.07 and beta=0.5
      12. test_ntxent_hcl_cuda (skip if no CUDA) -- same as beta_zero test but on GPU

      All tests import from dynaclr.loss import NTXentHCL.
      Run: uv run --package dynaclr pytest applications/dynaclr/tests/test_loss.py -- ALL MUST FAIL.
      Commit: test(23-01): add failing tests for NTXentHCL

    GREEN phase:
      Create loss.py:

      ```python
      from pytorch_metric_learning.losses import NTXentLoss
      import torch
      import torch.nn.functional as F
      from torch import Tensor

      class NTXentHCL(NTXentLoss):
          """NT-Xent loss with hard-negative concentration.

          When beta=0.0, produces identical results to standard NTXentLoss.
          When beta>0, up-weights hard negatives (high cosine similarity)
          in the denominator, focusing learning on difficult examples.

          Parameters
          ----------
          temperature : float
              Temperature scaling for cosine similarities. Default: 0.07.
          beta : float
              Hard-negative concentration strength. 0.0 = standard NT-Xent.
              Higher values concentrate more on hard negatives. Default: 0.5.
          """

          def __init__(self, temperature: float = 0.07, beta: float = 0.5):
              super().__init__(temperature=temperature)
              self.beta = beta

          def forward(self, embeddings: Tensor, labels: Tensor) -> Tensor:
              if self.beta == 0.0:
                  return super().forward(embeddings, labels)

              # Custom HCL implementation
              embeddings_normalized = F.normalize(embeddings, p=2, dim=1)
              sim_matrix = torch.mm(embeddings_normalized, embeddings_normalized.t()) / self.temperature

              n = embeddings.size(0)
              # Build positive mask: labels[i] == labels[j]
              labels_col = labels.unsqueeze(1)
              positive_mask = (labels_col == labels.unsqueeze(0)).float()
              # Remove self-similarity from positive mask
              self_mask = torch.eye(n, device=embeddings.device)
              positive_mask = positive_mask - self_mask
              # Negative mask: not self, not positive
              negative_mask = 1.0 - positive_mask - self_mask

              # HCL reweighting: weight negatives by exp(beta * sim)
              # Use sim before temperature scaling for the reweighting
              sim_for_reweight = torch.mm(embeddings_normalized, embeddings_normalized.t())
              neg_weights = torch.exp(self.beta * sim_for_reweight) * negative_mask
              # Normalize weights per row
              neg_weights = neg_weights / (neg_weights.sum(dim=1, keepdim=True) + 1e-8)
              # Scale back to count of negatives for proper loss magnitude
              num_negatives = negative_mask.sum(dim=1, keepdim=True)
              neg_weights = neg_weights * num_negatives

              # Weighted negative logits
              exp_sim = torch.exp(sim_matrix)
              weighted_neg_sum = (neg_weights * exp_sim).sum(dim=1)
              pos_sum = (positive_mask * exp_sim).sum(dim=1)

              # Loss: -log(pos / (pos + weighted_neg))
              loss = -torch.log(pos_sum / (pos_sum + weighted_neg_sum + 1e-8) + 1e-8)
              return loss.mean()
      ```

      NOTE: The exact HCL implementation above is a starting point. The key mathematical property that MUST hold is: when beta=0.0, the neg_weights become uniform (since exp(0 * sim) = 1 for all), making weighted_neg_sum == unweighted_neg_sum, producing identical results to standard NT-Xent. Adjust the implementation during GREEN phase to make test_ntxent_hcl_beta_zero_matches_standard pass with atol=1e-6.

      The super().forward() path for beta=0.0 is a fast path that guarantees exact numerical identity.

      Run: uv run --package dynaclr pytest applications/dynaclr/tests/test_loss.py -- ALL MUST PASS.
      Commit: feat(23-01): implement NTXentHCL with hard-negative concentration

    REFACTOR phase:
      - Add NTXentHCL to applications/dynaclr/src/dynaclr/__init__.py exports and __all__
      - Verify: uv run --package dynaclr python -c "from dynaclr import NTXentHCL; print('OK')"
      - Run full dynaclr test suite: uv run --package dynaclr pytest applications/dynaclr/tests/ -v
      - Lint: uv run ruff check applications/dynaclr/src/dynaclr/loss.py
      Commit: refactor(23-01): add NTXentHCL to package exports
  </implementation>
</feature>

<verification>
cd /Users/eduardo.hirata/Documents/repos/VisCy && uv run --package dynaclr pytest applications/dynaclr/tests/test_loss.py -v
All tests pass. No ruff lint errors: uv run ruff check applications/dynaclr/src/dynaclr/loss.py
Verify drop-in: uv run --package dynaclr python -c "from pytorch_metric_learning.losses import NTXentLoss; from dynaclr.loss import NTXentHCL; assert isinstance(NTXentHCL(), NTXentLoss); print('drop-in OK')"
Verify export: uv run --package dynaclr python -c "from dynaclr import NTXentHCL; print('OK')"
</verification>

<success_criteria>
- NTXentHCL importable from dynaclr.loss and from dynaclr (top-level)
- isinstance(NTXentHCL(), NTXentLoss) is True -- engine's isinstance check passes
- beta=0.0 produces numerically identical results to NTXentLoss (atol=1e-6)
- beta>0 produces different (higher) loss on hard negatives
- loss.backward() works, gradients flow
- All 12 tests pass, no lint errors
</success_criteria>

<output>
After completion, create `.planning/phases/23-loss-augmentation/23-01-SUMMARY.md`
</output>
