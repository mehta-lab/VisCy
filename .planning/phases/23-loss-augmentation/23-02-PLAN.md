---
phase: 23-loss-augmentation
plan: 02
type: tdd
wave: 1
depends_on: []
files_modified:
  - packages/viscy-data/src/viscy_data/channel_dropout.py
  - packages/viscy-data/tests/test_channel_dropout.py
  - packages/viscy-data/src/viscy_data/__init__.py
  - applications/dynaclr/src/dynaclr/tau_sampling.py
  - applications/dynaclr/tests/test_tau_sampling.py
  - applications/dynaclr/src/dynaclr/__init__.py
autonomous: true

must_haves:
  truths:
    - "ChannelDropout zeros specified channels with configurable probability on (B,C,Z,Y,X) tensors"
    - "ChannelDropout with p=0.0 never drops any channel and with p=1.0 always drops"
    - "ChannelDropout integrates after the existing scatter/gather augmentation chain in on_after_batch_transfer"
    - "ChannelDropout is importable from viscy_data (top-level package export)"
    - "sample_tau with exponential decay favors small temporal offsets -- median sample is closer to tau_min than midpoint"
    - "sample_tau returns integers within [tau_min, tau_max] inclusive"
  artifacts:
    - path: "packages/viscy-data/src/viscy_data/channel_dropout.py"
      provides: "ChannelDropout nn.Module for GPU augmentation pipeline"
      exports: ["ChannelDropout"]
      min_lines: 40
    - path: "packages/viscy-data/tests/test_channel_dropout.py"
      provides: "TDD test suite for ChannelDropout"
      min_lines: 80
    - path: "applications/dynaclr/src/dynaclr/tau_sampling.py"
      provides: "Exponential decay tau sampling utility"
      exports: ["sample_tau"]
      min_lines: 30
    - path: "applications/dynaclr/tests/test_tau_sampling.py"
      provides: "TDD test suite for variable tau sampling"
      min_lines: 50
  key_links:
    - from: "packages/viscy-data/tests/test_channel_dropout.py"
      to: "packages/viscy-data/src/viscy_data/channel_dropout.py"
      via: "from viscy_data.channel_dropout import ChannelDropout"
      pattern: "from viscy_data\\.channel_dropout import ChannelDropout"
    - from: "packages/viscy-data/src/viscy_data/__init__.py"
      to: "packages/viscy-data/src/viscy_data/channel_dropout.py"
      via: "top-level re-export"
      pattern: "from viscy_data\\.channel_dropout import ChannelDropout"
    - from: "applications/dynaclr/tests/test_tau_sampling.py"
      to: "applications/dynaclr/src/dynaclr/tau_sampling.py"
      via: "from dynaclr.tau_sampling import sample_tau"
      pattern: "from dynaclr\\.tau_sampling import sample_tau"
---

<objective>
TDD implementation of ChannelDropout augmentation (AUG-01, AUG-02) and variable tau sampling utility (AUG-03).

Purpose: ChannelDropout enables regularization by randomly zeroing the fluorescence channel during training, forcing the model to learn from phase contrast alone. Variable tau sampling with exponential decay biases temporal positive selection toward small offsets, aligning with the biological prior that nearby timepoints are more informative for learning dynamics.

Output: `channel_dropout.py` in viscy-data, `tau_sampling.py` in dynaclr, plus comprehensive TDD test suites and package exports.
</objective>

<execution_context>
@/Users/eduardo.hirata/.claude/get-shit-done/workflows/execute-plan.md
@/Users/eduardo.hirata/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@packages/viscy-data/src/viscy_data/gpu_aug.py
@packages/viscy-data/src/viscy_data/_utils.py
@packages/viscy-data/src/viscy_data/triplet.py
@packages/viscy-data/src/viscy_data/__init__.py
@applications/dynaclr/src/dynaclr/__init__.py
</context>

<feature>
  <name>ChannelDropout augmentation and variable tau sampling</name>
  <files>
    packages/viscy-data/src/viscy_data/channel_dropout.py
    packages/viscy-data/tests/test_channel_dropout.py
    applications/dynaclr/src/dynaclr/tau_sampling.py
    applications/dynaclr/tests/test_tau_sampling.py
  </files>
  <behavior>
    --- ChannelDropout ---

    ChannelDropout(channels: list[int], p: float = 0.5) is a torch.nn.Module.

    Operates on batched 5D tensors of shape (B, C, Z, Y, X):
    - During training: for each sample in the batch independently, with probability p, zeros out the specified channel indices
    - During eval: never drops (identity transform)
    - The dropout decision is per-sample, not per-batch (each sample in the batch independently has probability p of dropping)

    Integration point: Applied AFTER the scatter/gather augmentation chain in on_after_batch_transfer.
    The existing pipeline does: scatter channels -> per-channel transforms -> gather back to (B,C,Z,Y,X).
    ChannelDropout operates on the gathered (B,C,Z,Y,X) tensor, zeroing entire channels.

    Cases:
    - channels=[1], p=0.5, input (4,2,8,64,64) -> some samples have channel 1 zeroed, others intact
    - channels=[1], p=0.0 -> output == input (never drops)
    - channels=[1], p=1.0 -> all samples have channel 1 zeroed
    - channels=[0,1], p=1.0 -> both channels zeroed for all samples
    - eval mode -> output == input regardless of p
    - Works on CUDA tensors (in-place masking, no CPU roundtrip)
    - Preserves tensor dtype and device
    - Does not modify the input tensor (returns a clone or new tensor)

    --- Variable Tau Sampling ---

    sample_tau(tau_min: int, tau_max: int, rng: numpy.random.Generator, decay_rate: float = 2.0) -> int

    Draws a single tau value from the range [tau_min, tau_max] using exponential decay weighting:
    - Probabilities: p(tau) proportional to exp(-decay_rate * (tau - tau_min) / (tau_max - tau_min))
    - This makes small tau values (near tau_min) more likely than large ones
    - When decay_rate=0.0: uniform distribution (all tau equally likely)
    - Returns an integer in [tau_min, tau_max] inclusive

    Cases:
    - tau_min=1, tau_max=10, decay_rate=2.0, N=10000 samples -> median < midpoint (5.5)
    - tau_min=1, tau_max=10, decay_rate=0.0, N=10000 samples -> mean approximately 5.5
    - tau_min=1, tau_max=1 -> always returns 1
    - tau_min=1, tau_max=10, decay_rate=5.0 -> strongly favors tau_min (>50% of samples are 1 or 2)
    - Deterministic: same rng seed -> same sequence of tau values
    - All returned values are in [tau_min, tau_max] inclusive
  </behavior>
  <implementation>
    RED phase (both features):

      Create test_channel_dropout.py:
      1. test_channel_dropout_zeros_specified_channel -- p=1.0, channels=[1], verify channel 1 is all zeros
      2. test_channel_dropout_preserves_other_channels -- p=1.0, channels=[1], verify channel 0 is unchanged
      3. test_channel_dropout_p_zero_identity -- p=0.0, output equals input
      4. test_channel_dropout_p_one_always_drops -- p=1.0, run multiple times, always drops
      5. test_channel_dropout_probabilistic -- p=0.5, run 100 times, expect ~50% dropout rate (tolerance 20-80%)
      6. test_channel_dropout_eval_mode_identity -- model.eval(), output equals input
      7. test_channel_dropout_per_sample_independent -- batch of 16, p=0.5, not all samples have same dropout pattern
      8. test_channel_dropout_preserves_dtype_device -- float32 in, float32 out, same device
      9. test_channel_dropout_does_not_modify_input -- input tensor unchanged after forward pass
      10. test_channel_dropout_multiple_channels -- channels=[0,1], p=1.0, both zeroed
      11. test_channel_dropout_cuda (skip if no CUDA) -- works on GPU tensors

      Create test_tau_sampling.py:
      1. test_sample_tau_within_range -- all samples in [tau_min, tau_max]
      2. test_sample_tau_exponential_favors_small -- decay_rate=2.0, N=10000, median < midpoint
      3. test_sample_tau_uniform_when_zero_decay -- decay_rate=0.0, N=10000, mean approximately midpoint (tolerance 0.5)
      4. test_sample_tau_single_value -- tau_min == tau_max, always returns that value
      5. test_sample_tau_strong_decay -- decay_rate=5.0, >50% of 10000 samples are tau_min or tau_min+1
      6. test_sample_tau_deterministic -- same seed produces same sequence
      7. test_sample_tau_returns_int -- return type is int (not numpy int64)

      Run both test files -- ALL MUST FAIL.
      Commit: test(23-02): add failing tests for ChannelDropout and variable tau sampling

    GREEN phase:

      Create channel_dropout.py:
      ```python
      import torch
      from torch import Tensor, nn


      class ChannelDropout(nn.Module):
          """Randomly zero out entire channels during training.

          Designed for (B, C, Z, Y, X) tensors in the GPU augmentation pipeline.
          Applied after the scatter/gather augmentation chain in on_after_batch_transfer.

          Parameters
          ----------
          channels : list[int]
              Channel indices to potentially drop.
          p : float
              Probability of dropping each specified channel per sample. Default: 0.5.
          """

          def __init__(self, channels: list[int], p: float = 0.5) -> None:
              super().__init__()
              self.channels = channels
              self.p = p

          def forward(self, x: Tensor) -> Tensor:
              if not self.training or self.p == 0.0:
                  return x
              out = x.clone()
              B = out.shape[0]
              for ch in self.channels:
                  # Per-sample dropout mask
                  mask = torch.rand(B, device=out.device) < self.p
                  # Zero out channel ch for selected samples
                  # mask shape: (B,), index into batch dimension
                  out[mask, ch] = 0.0
              return out
      ```

      Create tau_sampling.py:
      ```python
      import numpy as np


      def sample_tau(
          tau_min: int,
          tau_max: int,
          rng: np.random.Generator,
          decay_rate: float = 2.0,
      ) -> int:
          """Sample a temporal offset using exponential decay.

          Probabilities are proportional to exp(-decay_rate * (tau - tau_min) / (tau_max - tau_min)),
          favoring small temporal offsets near tau_min.

          Parameters
          ----------
          tau_min : int
              Minimum tau value (inclusive).
          tau_max : int
              Maximum tau value (inclusive).
          rng : numpy.random.Generator
              Random number generator for reproducibility.
          decay_rate : float
              Exponential decay rate. 0.0 = uniform. Higher = stronger bias toward tau_min. Default: 2.0.

          Returns
          -------
          int
              Sampled tau value in [tau_min, tau_max].
          """
          if tau_min == tau_max:
              return int(tau_min)
          taus = np.arange(tau_min, tau_max + 1)
          weights = np.exp(-decay_rate * (taus - tau_min) / (tau_max - tau_min))
          weights /= weights.sum()
          return int(rng.choice(taus, p=weights))
      ```

      Run both test files -- ALL MUST PASS.
      Commit: feat(23-02): implement ChannelDropout and variable tau sampling

    REFACTOR phase:
      - Add ChannelDropout to packages/viscy-data/src/viscy_data/__init__.py exports and __all__
      - Add sample_tau to applications/dynaclr/src/dynaclr/__init__.py exports and __all__
      - Verify imports:
        uv run --package viscy-data python -c "from viscy_data import ChannelDropout; print('OK')"
        uv run --package dynaclr python -c "from dynaclr import sample_tau; print('OK')"
      - Run full test suites:
        uv run --package viscy-data pytest packages/viscy-data/tests/test_channel_dropout.py -v
        uv run --package dynaclr pytest applications/dynaclr/tests/test_tau_sampling.py -v
      - Lint both:
        uv run ruff check packages/viscy-data/src/viscy_data/channel_dropout.py
        uv run ruff check applications/dynaclr/src/dynaclr/tau_sampling.py
      Commit: refactor(23-02): add ChannelDropout and sample_tau to package exports
  </implementation>
</feature>

<verification>
cd /Users/eduardo.hirata/Documents/repos/VisCy
uv run --package viscy-data pytest packages/viscy-data/tests/test_channel_dropout.py -v
uv run --package dynaclr pytest applications/dynaclr/tests/test_tau_sampling.py -v
uv run ruff check packages/viscy-data/src/viscy_data/channel_dropout.py
uv run ruff check applications/dynaclr/src/dynaclr/tau_sampling.py
uv run --package viscy-data python -c "from viscy_data import ChannelDropout; print('OK')"
uv run --package dynaclr python -c "from dynaclr import sample_tau; print('OK')"
</verification>

<success_criteria>
- ChannelDropout importable from viscy_data.channel_dropout and viscy_data (top-level)
- p=0.0 is identity, p=1.0 always drops, p=0.5 is stochastic
- eval mode is identity regardless of p
- Per-sample independent dropout on (B,C,Z,Y,X) tensors
- sample_tau importable from dynaclr.tau_sampling and dynaclr (top-level)
- Exponential decay favors small tau values (statistical test passes)
- decay_rate=0.0 yields uniform distribution
- All 18 tests pass across both test files, no lint errors
</success_criteria>

<output>
After completion, create `.planning/phases/23-loss-augmentation/23-02-SUMMARY.md`
</output>
