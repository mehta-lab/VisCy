---
phase: 18-training-validation
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - applications/dynaclr/tests/test_training_integration.py
autonomous: true
requirements:
  - TRAIN-01
  - TRAIN-02

must_haves:
  truths:
    - "ContrastiveModule completes a fast_dev_run training loop (1 train batch + 1 val batch) without errors"
    - "YAML config class_path strings (dynaclr.engine.ContrastiveModule, viscy_models.contrastive.ContrastiveEncoder, viscy_data.triplet.TripletDataModule, viscy_transforms.*) all resolve to importable classes"
    - "The training test uses synthetic data matching TripletSample TypedDict format (anchor, positive, negative tensors + TrackingIndex)"
  artifacts:
    - path: "applications/dynaclr/tests/test_training_integration.py"
      provides: "Training integration test and config resolution test"
      min_lines: 80
  key_links:
    - from: "applications/dynaclr/tests/test_training_integration.py"
      to: "applications/dynaclr/src/dynaclr/engine.py"
      via: "ContrastiveModule import and fast_dev_run fit"
      pattern: "ContrastiveModule.*Trainer.*fast_dev_run"
    - from: "applications/dynaclr/tests/test_training_integration.py"
      to: "applications/dynaclr/examples/configs/fit.yml"
      via: "YAML parsing and class_path resolution"
      pattern: "class_path.*importlib|resolve"
---

<objective>
Create a training integration test that proves ContrastiveModule completes a full fast_dev_run loop with synthetic data, and verify that all YAML config class_path references resolve to real importable classes.

Purpose: This is the core validation that the modular DynaCLR application can actually train, not just import. Without this, we only have smoke tests for init/forward but no proof the Lightning training loop works end-to-end.

Output: `applications/dynaclr/tests/test_training_integration.py` with passing tests runnable via `uv run --package dynaclr pytest`
</objective>

<execution_context>
@/home/eduardo.hirata/.claude/get-shit-done/workflows/execute-plan.md
@/home/eduardo.hirata/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

# Key source files
@applications/dynaclr/src/dynaclr/engine.py
@applications/dynaclr/tests/test_engine.py
@applications/dynaclr/examples/configs/fit.yml
@applications/dynaclr/examples/configs/predict.yml
@packages/viscy-data/src/viscy_data/_typing.py
@applications/dynaclr/pyproject.toml
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create fast_dev_run training integration test for ContrastiveModule</name>
  <files>applications/dynaclr/tests/test_training_integration.py</files>
  <action>
Create `applications/dynaclr/tests/test_training_integration.py` with a training integration test. The approach:

1. **Create a SimpleEncoder** (reuse pattern from test_engine.py) — a small `nn.Module` with `forward(x)` returning `(features, projections)`. Use `nn.Linear` layers. Input: flatten 5D tensor to 1D. Output: features (batch, 64), projections (batch, 32).

2. **Create a SyntheticTripletDataModule** — a `LightningDataModule` subclass that:
   - In `train_dataloader()` and `val_dataloader()`, returns a `DataLoader` wrapping a simple `Dataset`
   - The dataset returns `TripletSample` dicts with keys: `anchor`, `positive`, `negative` (each `torch.Tensor` of shape `(C, D, H, W)` matching `example_input_array_shape` minus batch dim), and `index` (a `TrackingIndex` dict with `fov_name: str` and `id: int`)
   - Use small dimensions: C=1, D=1, H=1, W=10 (matching the SimpleEncoder's expected flattened input of 10)
   - Dataset size: 4 samples (enough for 1 batch with batch_size=2)

3. **Write `test_contrastive_fast_dev_run()`**:
   - Create `SimpleEncoder`
   - Create `ContrastiveModule(encoder=encoder, loss_function=nn.TripletMarginLoss(margin=0.5), lr=1e-3, example_input_array_shape=(1, 1, 1, 1, 10))`
   - Create `SyntheticTripletDataModule`
   - Create `Trainer(fast_dev_run=True, accelerator="cpu", logger=False, enable_checkpointing=False)` — use `logger=False` to avoid the `_log_samples` call to `self.logger.experiment` which would fail without a real TensorBoard logger. The `on_train_epoch_end` calls `_log_samples` which calls `self.logger.experiment.add_image` — with `logger=False`, `self.logger` is `None` so this will raise. To handle this cleanly, use `logger=TensorBoardLogger(save_dir=tmp_path)` instead (import from `lightning.pytorch.loggers`), using pytest's `tmp_path` fixture.
   - Call `trainer.fit(module, datamodule=datamodule)`
   - Assert `trainer.state.finished is True`
   - Assert `trainer.state.status == "finished"`

4. **Write `test_contrastive_ntxent_fast_dev_run()`**:
   - Same as above but with `NTXentLoss()` as the loss function (no negative needed for NTXent, but the data module can still provide it — the training_step checks `isinstance(self.loss_function, NTXentLoss)` and ignores negative)
   - Import `from pytorch_metric_learning.losses import NTXentLoss`
   - This tests the NTXent code path in training_step

5. **Write `test_config_class_paths_resolve()`** (addresses TRAIN-02):
   - Parse `applications/dynaclr/examples/configs/fit.yml` and `predict.yml` using PyYAML
   - Extract all `class_path` values recursively from the parsed dict
   - For each class_path, split into module and class name, use `importlib.import_module` + `getattr` to verify the class exists
   - Assert all class_paths resolve without ImportError
   - Use `pathlib.Path(__file__).parents[2] / "examples" / "configs"` to locate the YAML files relative to the test file

Important implementation details:
- The `_log_samples` method in `on_train_epoch_end` calls `render_images` which returns an ndarray, then `self.logger.experiment.add_image`. With a TensorBoardLogger this works. Use `tmp_path` fixture for the logger's save_dir.
- TripletSample is a TypedDict: `anchor: Tensor, positive: NotRequired[Tensor], negative: NotRequired[Tensor], index: NotRequired[TrackingIndex]`
- TrackingIndex is a TypedDict: `fov_name: OneOrSeq[str], id: OneOrSeq[int]`
- The training_step accesses `batch["negative"]` in the non-NTXent branch, so the synthetic data MUST include the negative key for the TripletMarginLoss test.
- Use `enable_progress_bar=False` in Trainer to keep test output clean.
  </action>
  <verify>
Run:
```bash
cd /hpc/mydata/eduardo.hirata/repos/viscy && uv run --package dynaclr pytest applications/dynaclr/tests/test_training_integration.py -v
```
All 3 tests must pass (test_contrastive_fast_dev_run, test_contrastive_ntxent_fast_dev_run, test_config_class_paths_resolve).
  </verify>
  <done>
- `test_contrastive_fast_dev_run` passes: ContrastiveModule with TripletMarginLoss completes fast_dev_run (train + val batch) on CPU
- `test_contrastive_ntxent_fast_dev_run` passes: ContrastiveModule with NTXentLoss completes fast_dev_run
- `test_config_class_paths_resolve` passes: All class_path strings in fit.yml and predict.yml resolve to importable Python classes
- Full test suite still passes: `uv run --package dynaclr pytest` runs all tests including existing test_engine.py
  </done>
</task>

</tasks>

<verification>
1. `uv run --package dynaclr pytest applications/dynaclr/tests/ -v` — all tests pass (existing smoke tests + new integration tests)
2. Training integration tests exercise the full Lightning training loop (training_step, validation_step, on_train_epoch_end, on_validation_epoch_end, configure_optimizers)
3. Config class_path test covers both fit.yml and predict.yml, verifying every class_path reference
</verification>

<success_criteria>
- `uv run --package dynaclr pytest` discovers and runs the training integration test
- fast_dev_run completes all stages (train batch, validation batch) without errors
- YAML config class_paths all resolve to importable classes
- No changes to production code (only test additions)
</success_criteria>

<output>
After completion, create `.planning/phases/18-training-validation/18-01-SUMMARY.md`
</output>
