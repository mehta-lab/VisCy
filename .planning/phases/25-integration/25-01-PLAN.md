---
phase: 25-integration
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - applications/dynaclr/tests/test_multi_experiment_integration.py
  - applications/dynaclr/examples/configs/multi_experiment_fit.yml
autonomous: true

must_haves:
  truths:
    - "A fast_dev_run integration test completes without errors using MultiExperimentDataModule + ContrastiveModule + NTXentHCL with 2 synthetic experiments having different channel sets"
    - "A YAML config example for multi-experiment training with all sampling axes (experiment_aware, condition_balanced, temporal_enrichment) exists and is parseable by Lightning CLI class_path resolution"
  artifacts:
    - path: "applications/dynaclr/tests/test_multi_experiment_integration.py"
      provides: "End-to-end multi-experiment training integration test"
      min_lines: 120
    - path: "applications/dynaclr/examples/configs/multi_experiment_fit.yml"
      provides: "YAML config example for multi-experiment DynaCLR training"
      min_lines: 60
  key_links:
    - from: "applications/dynaclr/tests/test_multi_experiment_integration.py"
      to: "dynaclr.datamodule.MultiExperimentDataModule"
      via: "import and instantiation with experiments_yaml"
      pattern: "MultiExperimentDataModule"
    - from: "applications/dynaclr/tests/test_multi_experiment_integration.py"
      to: "dynaclr.engine.ContrastiveModule"
      via: "import and instantiation with NTXentHCL loss"
      pattern: "ContrastiveModule.*NTXentHCL"
    - from: "applications/dynaclr/tests/test_multi_experiment_integration.py"
      to: "lightning.pytorch.Trainer"
      via: "fast_dev_run=True fit call"
      pattern: "Trainer.*fast_dev_run"
    - from: "applications/dynaclr/examples/configs/multi_experiment_fit.yml"
      to: "dynaclr.datamodule.MultiExperimentDataModule"
      via: "class_path reference"
      pattern: "class_path.*MultiExperimentDataModule"
    - from: "applications/dynaclr/examples/configs/multi_experiment_fit.yml"
      to: "dynaclr.loss.NTXentHCL"
      via: "class_path reference"
      pattern: "class_path.*NTXentHCL"
---

<objective>
Create an end-to-end integration test and YAML config example that validate the full multi-experiment DynaCLR training pipeline.

Purpose: This is the capstone of the v2.2 Composable Sampling Framework milestone. It proves that all components (ExperimentRegistry, MultiExperimentIndex, MultiExperimentTripletDataset, MultiExperimentDataModule, FlexibleBatchSampler, NTXentHCL, ChannelDropout) work together in a real Lightning training loop.

Output: A passing integration test and a reference YAML config that users can adapt for their own multi-experiment training.
</objective>

<execution_context>
@/Users/eduardo.hirata/.claude/get-shit-done/workflows/execute-plan.md
@/Users/eduardo.hirata/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/24-dataset-datamodule/24-02-SUMMARY.md
@.planning/phases/23-loss-augmentation/23-01-SUMMARY.md
@.planning/phases/18-training-validation/18-01-SUMMARY.md

# Key source files to reference during implementation:
@applications/dynaclr/tests/test_training_integration.py  # Phase 18 pattern: SimpleEncoder, fast_dev_run, TensorBoardLogger
@applications/dynaclr/tests/test_datamodule.py  # Phase 24 pattern: _create_experiment, _write_experiments_yaml, synthetic zarr
@applications/dynaclr/src/dynaclr/datamodule.py  # MultiExperimentDataModule interface
@applications/dynaclr/src/dynaclr/engine.py  # ContrastiveModule training_step with NTXentLoss isinstance check
@applications/dynaclr/src/dynaclr/loss.py  # NTXentHCL (subclass of NTXentLoss)
@applications/dynaclr/examples/configs/fit.yml  # Existing single-experiment config pattern
@applications/dynaclr/examples/configs/experiments.yml  # Experiment YAML format
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create end-to-end multi-experiment fast_dev_run integration test</name>
  <files>applications/dynaclr/tests/test_multi_experiment_integration.py</files>
  <action>
Create `applications/dynaclr/tests/test_multi_experiment_integration.py` that exercises the FULL multi-experiment DynaCLR training pipeline end-to-end.

**Test setup (reuse pattern from test_datamodule.py):**
- Create 2 synthetic experiments with DIFFERENT channel sets to prove cross-experiment channel alignment:
  - Experiment "exp_alpha": channel_names=["Phase3D", "GFP", "Mito"], source_channel=["Phase3D", "GFP"]
  - Experiment "exp_beta": channel_names=["Phase3D", "RFP", "StressGranules"], source_channel=["Phase3D", "RFP"]
  - This is the key multi-experiment scenario: same positional alignment (position 0 = phase, position 1 = fluor) but different channel names
- Each experiment: 1 well, 1 FOV, 5 tracks, 10 timepoints
- Small image: 64x64 YX, 1 Z, 2 source channels
- Write experiments.yaml via helper function
- condition_wells: {"control": ["A/1"]} and {"control": ["B/1"]} respectively

**SimpleEncoder (reuse from test_training_integration.py):**
- nn.Module with fc + proj layers
- Input: (B, C=2, Z=1, Y=24, X=24) -> flatten -> fc -> proj
- C=2 because 2 source channels; Z=1, Y=24, X=24 matches final_yx_patch_size
- Output: (features, projections) tuple

**Test function `test_multi_experiment_fast_dev_run(tmp_path)`:**
1. Create 2 synthetic experiments via helpers
2. Write experiments YAML
3. Instantiate MultiExperimentDataModule with:
   - experiments_yaml=str(yaml_path)
   - z_range=(0, 1)
   - yx_patch_size=(32, 32)
   - final_yx_patch_size=(24, 24)
   - val_experiments=["exp_beta"]
   - tau_range=(0.5, 2.0)
   - batch_size=4 (small for fast test)
   - num_workers=1 (ThreadDataLoader requires at least 1 worker)
   - experiment_aware=True
   - condition_balanced=False (single condition per experiment)
   - temporal_enrichment=False (keep simple for integration test)
   - channel_dropout_channels=[1]
   - channel_dropout_prob=0.5
4. Instantiate ContrastiveModule with:
   - encoder=SimpleEncoder()
   - loss_function=NTXentHCL(temperature=0.07, beta=0.5) -- proves HCL loss works end-to-end
   - lr=1e-3
   - example_input_array_shape=(1, 2, 1, 24, 24)
5. Instantiate Trainer with:
   - fast_dev_run=True
   - accelerator="cpu"
   - logger=TensorBoardLogger(save_dir=tmp_path)
   - enable_checkpointing=False
   - enable_progress_bar=False
6. Call trainer.fit(module, datamodule=datamodule)
7. Assert trainer.state.finished is True
8. Assert trainer.state.status == "finished"

**Additional test `test_multi_experiment_fast_dev_run_with_all_sampling_axes(tmp_path)`:**
- Same setup but with ALL sampling axes enabled:
  - experiment_aware=True
  - condition_balanced=True (requires 2 conditions per experiment)
  - temporal_enrichment=True
  - temporal_window_hours=2.0
  - temporal_global_fraction=0.3
- This requires modifying the fixture to have 2 conditions per experiment:
  - exp_alpha: condition_wells={"uninfected": ["A/1"], "infected": ["A/2"]} with 2 wells
  - exp_beta: condition_wells={"uninfected": ["B/1"], "infected": ["B/2"]} with 2 wells
- Also need hours_post_infection column -- this comes from MultiExperimentIndex which computes it from start_hpi + t * interval_minutes/60
- Set start_hpi=0.0 on both experiments so HPI = t * interval_minutes/60
- Verifies the full sampling cascade works end-to-end

**IMPORTANT implementation details:**
- NTXentHCL is a subclass of NTXentLoss, so `isinstance(NTXentHCL(...), NTXentLoss)` is True. This means ContrastiveModule.training_step will correctly take the NTXent code path (labels + embeddings).
- MultiExperimentDataModule's `collate_fn=lambda x: x` means batches arrive as-is from __getitems__ -- they're already dicts with stacked tensors.
- The on_after_batch_transfer chain: normalizations -> augmentations -> final_crop -> channel_dropout. With no normalizations/augmentations configured, only final_crop + channel_dropout apply.
- Use num_workers=1 for ThreadDataLoader (the DataModule default; ThreadDataLoader requires at least 1 worker).

**Synthetic data creation helpers (adapt from test_datamodule.py):**
- `_make_tracks_csv(path, n_tracks, n_t)` -- write CSV with track_id, t, id, parent_track_id, parent_id, z, y, x columns
- `_create_experiment(tmp_path, name, channel_names, source_channel, wells, condition_wells, ...)` -- create HCS OME-Zarr store + tracks CSVs + return ExperimentConfig
- `_write_experiments_yaml(tmp_path, configs)` -- write YAML file from configs

Use `from iohub.ngff import open_ome_zarr` for Zarr store creation.
Use `numpy.random.default_rng(42)` for deterministic synthetic data.

Patch size math: yx_patch_size=(32,32) is the initial extraction size. final_yx_patch_size=(24,24) is the output after center crop. Image must be at least 32x32 so patches can be extracted. Cell centroids at (32, 32) with 64x64 image and 32x32 patch -> valid.
  </action>
  <verify>
Run: `uv run --package dynaclr pytest applications/dynaclr/tests/test_multi_experiment_integration.py -v`

Expected: All tests pass (2 tests: test_multi_experiment_fast_dev_run, test_multi_experiment_fast_dev_run_with_all_sampling_axes).
  </verify>
  <done>
Two fast_dev_run integration tests pass that exercise MultiExperimentDataModule + ContrastiveModule + NTXentHCL with 2 synthetic experiments having different channel sets (GFP vs RFP). The second test additionally enables all sampling axes (experiment_aware + condition_balanced + temporal_enrichment).
  </done>
</task>

<task type="auto">
  <name>Task 2: Create multi-experiment YAML config example with class_path validation test</name>
  <files>
    applications/dynaclr/examples/configs/multi_experiment_fit.yml
    applications/dynaclr/tests/test_multi_experiment_integration.py
  </files>
  <action>
**Part A: Create `applications/dynaclr/examples/configs/multi_experiment_fit.yml`:**

A complete Lightning CLI YAML config for multi-experiment DynaCLR training. Model after the existing `fit.yml` but replace TripletDataModule with MultiExperimentDataModule and TripletMarginLoss with NTXentHCL.

Structure:
```yaml
# Multi-experiment DynaCLR training configuration
# ================================================
# This config demonstrates training with MultiExperimentDataModule
# and NTXentHCL loss across multiple experiments with different
# fluorescence reporters but shared phase contrast channel.
#
# Usage:
#   dynaclr fit --config multi_experiment_fit.yml
#
# Requires an experiments.yml file (see experiments.yml in this directory)
# with experiment definitions.

seed_everything: 42
trainer:
  accelerator: gpu
  strategy: ddp
  devices: 4
  num_nodes: 1
  precision: 32-true
  logger:
    class_path: lightning.pytorch.loggers.TensorBoardLogger
    init_args:
      save_dir: #TODO path to log directory
      version: #TODO version name
      log_graph: True
  callbacks:
    - class_path: lightning.pytorch.callbacks.LearningRateMonitor
      init_args:
        logging_interval: step
    - class_path: lightning.pytorch.callbacks.ModelCheckpoint
      init_args:
        monitor: loss/val
        every_n_epochs: 1
        save_top_k: 4
        save_last: true
  fast_dev_run: false
  max_epochs: 100
  log_every_n_steps: 10
  enable_checkpointing: true
  inference_mode: true
  use_distributed_sampler: false  # FlexibleBatchSampler handles DDP internally
model:
  class_path: dynaclr.engine.ContrastiveModule
  init_args:
    encoder:
      class_path: viscy_models.contrastive.ContrastiveEncoder
      init_args:
        backbone: convnext_tiny
        in_channels: 2
        in_stack_depth: 30
        stem_kernel_size: [5, 4, 4]
        stem_stride: [5, 4, 4]
        embedding_dim: 768
        projection_dim: 32
        drop_path_rate: 0.0
    loss_function:
      class_path: dynaclr.loss.NTXentHCL
      init_args:
        temperature: 0.07
        beta: 0.5
    lr: 0.00002
    log_batches_per_epoch: 3
    log_samples_per_batch: 3
    example_input_array_shape: [1, 2, 30, 256, 256]
data:
  class_path: dynaclr.datamodule.MultiExperimentDataModule
  init_args:
    experiments_yaml: #TODO path to experiments.yml
    z_range: [15, 45]
    yx_patch_size: [384, 384]
    final_yx_patch_size: [160, 160]
    val_experiments:
      - #TODO experiment name(s) for validation
    tau_range: [0.5, 2.0]
    tau_decay_rate: 2.0
    batch_size: 64
    num_workers: 12
    # Sampling axes
    experiment_aware: true
    condition_balanced: true
    leaky: 0.0
    temporal_enrichment: true
    temporal_window_hours: 2.0
    temporal_global_fraction: 0.3
    # Augmentation
    channel_dropout_channels: [1]  # Drop fluorescence channel
    channel_dropout_prob: 0.5
    normalizations:
      - class_path: viscy_transforms.NormalizeSampled
        init_args:
          keys: [ch_0]
          level: fov_statistics
          subtrahend: mean
          divisor: std
      - class_path: viscy_transforms.ScaleIntensityRangePercentilesd
        init_args:
          keys: [ch_1]
          lower: 50
          upper: 99
          b_min: 0.0
          b_max: 1.0
    augmentations:
      - class_path: viscy_transforms.RandAffined
        init_args:
          keys: [ch_0, ch_1]
          prob: 0.8
          scale_range: [0, 0.2, 0.2]
          rotate_range: [3.14, 0.0, 0.0]
          shear_range: [0.0, 0.01, 0.01]
          padding_mode: zeros
      - class_path: viscy_transforms.RandAdjustContrastd
        init_args:
          keys: [ch_1]
          prob: 0.5
          gamma: [0.8, 1.2]
      - class_path: viscy_transforms.RandAdjustContrastd
        init_args:
          keys: [ch_0]
          prob: 0.5
          gamma: [0.8, 1.2]
      - class_path: viscy_transforms.RandScaleIntensityd
        init_args:
          keys: [ch_1]
          prob: 0.5
          factors: 0.5
      - class_path: viscy_transforms.RandScaleIntensityd
        init_args:
          keys: [ch_0]
          prob: 0.5
          factors: 0.5
      - class_path: viscy_transforms.RandGaussianSmoothd
        init_args:
          keys: [ch_0, ch_1]
          prob: 0.5
          sigma_x: [0.25, 0.75]
          sigma_y: [0.25, 0.75]
          sigma_z: [0.0, 0.0]
      - class_path: viscy_transforms.RandGaussianNoised
        init_args:
          keys: [ch_1]
          prob: 0.5
          mean: 0.0
          std: 0.2
      - class_path: viscy_transforms.RandGaussianNoised
        init_args:
          keys: [ch_0]
          prob: 0.5
          mean: 0.0
          std: 0.2
    # Loss reference (informational -- actual loss is on model.loss_function)
    hcl_beta: 0.5
    cache_pool_bytes: 0
    seed: 0
```

**Key differences from fit.yml:**
1. `data.class_path` is `dynaclr.datamodule.MultiExperimentDataModule` (not `viscy_data.triplet.TripletDataModule`)
2. `loss_function.class_path` is `dynaclr.loss.NTXentHCL` (not `torch.nn.TripletMarginLoss`)
3. `use_distributed_sampler: false` -- FlexibleBatchSampler handles DDP internally
4. Normalizations and augmentations use generic `ch_0`, `ch_1` keys (not experiment-specific channel names)
5. All sampling axes configured: experiment_aware, condition_balanced, temporal_enrichment

**Part B: Add class_path validation to the integration test:**

In `test_multi_experiment_integration.py`, add a test `test_multi_experiment_config_class_paths_resolve()` that:
1. Loads `multi_experiment_fit.yml` from `examples/configs/`
2. Extracts all `class_path` values recursively
3. Verifies each resolves to an importable Python class
4. Reuse the `_extract_class_paths` and `_resolve_class_path` helpers from `test_training_integration.py` (copy them or import -- prefer copying to keep test self-contained)

This is the same pattern as `test_config_class_paths_resolve` in test_training_integration.py but for the new config.
  </action>
  <verify>
Run: `uv run --package dynaclr pytest applications/dynaclr/tests/test_multi_experiment_integration.py -v -k "class_paths"`

Expected: test_multi_experiment_config_class_paths_resolve passes (all class_paths in multi_experiment_fit.yml resolve to importable classes).

Also verify: `python -c "import yaml; yaml.safe_load(open('applications/dynaclr/examples/configs/multi_experiment_fit.yml'))"` succeeds (valid YAML).
  </verify>
  <done>
A multi_experiment_fit.yml config example exists in examples/configs/ demonstrating multi-experiment training with all sampling axes enabled, NTXentHCL loss, generic channel names, and all class_paths resolve to importable Python classes.
  </done>
</task>

</tasks>

<verification>
1. Run full integration test suite: `uv run --package dynaclr pytest applications/dynaclr/tests/test_multi_experiment_integration.py -v`
   - All 3 tests pass: fast_dev_run (basic), fast_dev_run (all sampling axes), config class_paths
2. Run full dynaclr test suite to verify no regressions: `uv run --package dynaclr pytest applications/dynaclr/tests/ -v --tb=short`
   - All existing tests still pass
3. Verify YAML config is valid: `python -c "import yaml; yaml.safe_load(open('applications/dynaclr/examples/configs/multi_experiment_fit.yml'))"`
4. Verify all class_paths in config resolve to importable classes
</verification>

<success_criteria>
- INTG-01: fast_dev_run integration test completes without errors using MultiExperimentDataModule + ContrastiveModule + NTXentHCL with 2 synthetic multi-experiment datasets having different channel sets
- INTG-02: multi_experiment_fit.yml config example demonstrates multi-experiment training with all sampling axes (experiment_aware, condition_balanced, temporal_enrichment) and all class_paths resolve to importable Python classes
</success_criteria>

<output>
After completion, create `.planning/phases/25-integration/25-01-SUMMARY.md`
</output>
